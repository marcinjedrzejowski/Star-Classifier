{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluateAndPrintMetricsRanking (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"utils/preprocessing.jl\")\n",
    "include(\"utils/model_evaluation.jl\")\n",
    "include(\"utils/data_loader.jl\")\n",
    "include(\"utils/visualization.jl\")\n",
    "include(\"utils/ml1_utils.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "Random.seed!(123)\n",
    "\n",
    "data = DataLoader.load_data_for_analysis(\"dataset\\\\star_classification.csv\");\n",
    "#Visualization.entry_visualization(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I've changed the preprocess_data function so it doesn't OneHotEncode the targets\\n    because it's not needed&advised for KNN, DT & SVM, only for ANN. \\n    The OneHotEncoding for the ANN will be done in the modelCrossValidation function,\\n    which is called by the evaluateAndPrintMetricsRanking function.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DataFrames\n",
    "\n",
    "data = DataLoader.load_data(\"dataset\\\\star_classification.csv\");\n",
    "\n",
    "# Preprocess the data\n",
    "\n",
    "\"\"\"    This function does the following:\n",
    "        - Balance the data using the undersampling method if chosen to do so\n",
    "        - Parse the data: chosing the correct columns for inputs and targets (Shouldn't this be done before balancing??)\n",
    "        - Splits the data into training and testing using holdOut method\n",
    "        - Normalize the inputs  ------> zeromean method still to be implemented!!!!\n",
    "\"\"\"\n",
    "# preprocess_data(data, train_ratio, norm_method, balance, indices)\n",
    "train_inputs, train_targets, test_inputs, test_targets = Preprocessing.preprocess_data(data, 0.98, \"minmax\", true, [4,5,6,7,8]);\n",
    "\n",
    "\"\"\" I've changed the preprocess_data function so it doesn't OneHotEncode the targets\n",
    "    because it's not needed&advised for KNN, DT & SVM, only for ANN. \n",
    "    The OneHotEncoding for the ANN will be done in the modelCrossValidation function,\n",
    "    which is called by the evaluateAndPrintMetricsRanking function.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix{Float32}Vector{Any}Matrix{Float32}Vector{Any}\n",
      "(1138, 5)(1138,)(55745, 5)(55745,)\n",
      "Any[\"STAR\", \"STAR\", \"QSO\", \"GALAXY\", \"QSO\", \"QSO\", \"STAR\", \"QSO\", \"GALAXY\", \"GALAXY\"]\n"
     ]
    }
   ],
   "source": [
    "println(typeof(train_inputs), typeof(train_targets),typeof(test_inputs), typeof(test_targets))\n",
    "println(size(train_inputs),size(train_targets),size(test_inputs),size(test_targets))\n",
    "println(train_targets[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inputs: (1138, 5)\n",
      "Train targets: (1138,)\n",
      "Test inputs: (55745, 5)\n",
      "Test targets: (55745,)\n"
     ]
    }
   ],
   "source": [
    "# Check size of train and test sets\n",
    "println(\"Train inputs: \", size(train_inputs))\n",
    "println(\"Train targets: \", size(train_targets))\n",
    "println(\"Test inputs: \", size(test_inputs))\n",
    "println(\"Test targets: \", size(test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing hyperparameters for each model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.neighbors._classification.KNeighborsClassifier'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "\n",
    "@sk_import neural_network: MLPClassifier\n",
    "@sk_import svm: SVC\n",
    "@sk_import tree: DecisionTreeClassifier\n",
    "@sk_import neighbors: KNeighborsClassifier\n",
    "@sk_import ensemble: StackingClassifier  # For the stacking ensemble\n",
    "@sk_import linear_model: LogisticRegression  # For the final estimator in stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setting indices for the k-fold cross-validation\n",
    "    we are about to do with the different models\n",
    "\"\"\"\n",
    "N=size(train_inputs,1)\n",
    "k = 5 # number of folds\n",
    "kFoldIndices = crossvalidation(N, k);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with set of hyperparameters 1\n",
      "Training with set of hyperparameters 2\n",
      "Training with set of hyperparameters 3\n",
      "Training with set of hyperparameters 4\n",
      "Training with set of hyperparameters 5\n",
      "Training with set of hyperparameters 6\n",
      "\n",
      "----- acc -----\n",
      "Set of hyperparameters 3 -> mean: 0.689 Std. Dev.: 0.021\n",
      "Set of hyperparameters 5 -> mean: 0.689 Std. Dev.: 0.026\n",
      "Set of hyperparameters 6 -> mean: 0.689 Std. Dev.: 0.026\n",
      "Set of hyperparameters 4 -> mean: 0.688 Std. Dev.: 0.025\n",
      "Set of hyperparameters 2 -> mean: 0.683 Std. Dev.: 0.032\n",
      "Set of hyperparameters 1 -> mean: 0.622 Std. Dev.: 0.037\n",
      "\n",
      "----- sensitivity -----\n",
      "Set of hyperparameters 3 -> mean: 0.689 Std. Dev.: 0.021\n",
      "Set of hyperparameters 5 -> mean: 0.689 Std. Dev.: 0.026\n",
      "Set of hyperparameters 6 -> mean: 0.689 Std. Dev.: 0.026\n",
      "Set of hyperparameters 4 -> mean: 0.688 Std. Dev.: 0.025\n",
      "Set of hyperparameters 2 -> mean: 0.683 Std. Dev.: 0.032\n",
      "Set of hyperparameters 1 -> mean: 0.622 Std. Dev.: 0.037\n",
      "\n",
      "----- specificity -----\n",
      "Set of hyperparameters 3 -> mean: 0.844 Std. Dev.: 0.009\n",
      "Set of hyperparameters 5 -> mean: 0.844 Std. Dev.: 0.012\n",
      "Set of hyperparameters 6 -> mean: 0.844 Std. Dev.: 0.012\n",
      "Set of hyperparameters 4 -> mean: 0.843 Std. Dev.: 0.012\n",
      "Set of hyperparameters 2 -> mean: 0.841 Std. Dev.: 0.014\n",
      "Set of hyperparameters 1 -> mean: 0.813 Std. Dev.: 0.016\n",
      "\n",
      "----- ppv -----\n",
      "Set of hyperparameters 5 -> mean: 0.692 Std. Dev.: 0.021\n",
      "Set of hyperparameters 6 -> mean: 0.692 Std. Dev.: 0.021\n",
      "Set of hyperparameters 4 -> mean: 0.691 Std. Dev.: 0.021\n",
      "Set of hyperparameters 3 -> mean: 0.69 Std. Dev.: 0.019\n",
      "Set of hyperparameters 2 -> mean: 0.685 Std. Dev.: 0.028\n",
      "Set of hyperparameters 1 -> mean: 0.629 Std. Dev.: 0.039\n",
      "\n",
      "----- npv -----\n",
      "Set of hyperparameters 3 -> mean: 0.846 Std. Dev.: 0.011\n",
      "Set of hyperparameters 2 -> mean: 0.846 Std. Dev.: 0.019\n",
      "Set of hyperparameters 5 -> mean: 0.844 Std. Dev.: 0.014\n",
      "Set of hyperparameters 6 -> mean: 0.844 Std. Dev.: 0.014\n",
      "Set of hyperparameters 4 -> mean: 0.844 Std. Dev.: 0.014\n",
      "Set of hyperparameters 1 -> mean: 0.813 Std. Dev.: 0.02\n",
      "\n",
      "----- f_score -----\n",
      "Set of hyperparameters 5 -> mean: 0.688 Std. Dev.: 0.022\n",
      "Set of hyperparameters 6 -> mean: 0.688 Std. Dev.: 0.022\n",
      "Set of hyperparameters 4 -> mean: 0.688 Std. Dev.: 0.022\n",
      "Set of hyperparameters 3 -> mean: 0.687 Std. Dev.: 0.018\n",
      "Set of hyperparameters 2 -> mean: 0.676 Std. Dev.: 0.025\n",
      "Set of hyperparameters 1 -> mean: 0.618 Std. Dev.: 0.038\n",
      "\n",
      "----- err_rate -----\n",
      "Set of hyperparameters 3 -> mean: 0.311 Std. Dev.: 0.021\n",
      "Set of hyperparameters 5 -> mean: 0.311 Std. Dev.: 0.026\n",
      "Set of hyperparameters 6 -> mean: 0.311 Std. Dev.: 0.026\n",
      "Set of hyperparameters 4 -> mean: 0.312 Std. Dev.: 0.025\n",
      "Set of hyperparameters 2 -> mean: 0.317 Std. Dev.: 0.032\n",
      "Set of hyperparameters 1 -> mean: 0.378 Std. Dev.: 0.037\n"
     ]
    }
   ],
   "source": [
    "# Define an array of hyperparameter dictionaries for the Decision Tree model\n",
    "dtree_hyperparameters_array = [\n",
    "    Dict(\"max_depth\" => 3),\n",
    "    Dict(\"max_depth\" => 5),\n",
    "    Dict(\"max_depth\" => 10),\n",
    "    Dict(\"max_depth\" => 20),\n",
    "    Dict(\"max_depth\" => 50),\n",
    "    Dict(\"max_depth\" => 100) # Deeper trees can capture more detail but risk overfitting\n",
    "]\n",
    "\n",
    "# Call the function to evaluate the model using different sets of hyperparameters and print the ranking of metrics.\n",
    "evaluateAndPrintMetricsRanking(:DecisionTree,dtree_hyperparameters_array, train_inputs, train_targets, kFoldIndices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with set of hyperparameters 1\n",
      "Training with set of hyperparameters 2\n",
      "Training with set of hyperparameters 3\n",
      "Training with set of hyperparameters 4\n",
      "Training with set of hyperparameters 5\n",
      "Training with set of hyperparameters 6\n",
      "\n",
      "----- acc -----\n",
      "Set of hyperparameters 1 -> mean: 0.742 Std. Dev.: 0.036\n",
      "Set of hyperparameters 2 -> mean: 0.726 Std. Dev.: 0.029\n",
      "Set of hyperparameters 3 -> mean: 0.712 Std. Dev.: 0.047\n",
      "Set of hyperparameters 4 -> mean: 0.709 Std. Dev.: 0.041\n",
      "Set of hyperparameters 5 -> mean: 0.699 Std. Dev.: 0.04\n",
      "Set of hyperparameters 6 -> mean: 0.681 Std. Dev.: 0.028\n",
      "\n",
      "----- sensitivity -----\n",
      "Set of hyperparameters 1 -> mean: 0.742 Std. Dev.: 0.036\n",
      "Set of hyperparameters 2 -> mean: 0.726 Std. Dev.: 0.029\n",
      "Set of hyperparameters 3 -> mean: 0.712 Std. Dev.: 0.047\n",
      "Set of hyperparameters 4 -> mean: 0.709 Std. Dev.: 0.041\n",
      "Set of hyperparameters 5 -> mean: 0.699 Std. Dev.: 0.04\n",
      "Set of hyperparameters 6 -> mean: 0.681 Std. Dev.: 0.028\n",
      "\n",
      "----- specificity -----\n",
      "Set of hyperparameters 1 -> mean: 0.869 Std. Dev.: 0.019\n",
      "Set of hyperparameters 2 -> mean: 0.861 Std. Dev.: 0.015\n",
      "Set of hyperparameters 3 -> mean: 0.854 Std. Dev.: 0.024\n",
      "Set of hyperparameters 4 -> mean: 0.853 Std. Dev.: 0.022\n",
      "Set of hyperparameters 5 -> mean: 0.849 Std. Dev.: 0.022\n",
      "Set of hyperparameters 6 -> mean: 0.84 Std. Dev.: 0.016\n",
      "\n",
      "----- ppv -----\n",
      "Set of hyperparameters 1 -> mean: 0.742 Std. Dev.: 0.039\n",
      "Set of hyperparameters 2 -> mean: 0.729 Std. Dev.: 0.032\n",
      "Set of hyperparameters 3 -> mean: 0.711 Std. Dev.: 0.051\n",
      "Set of hyperparameters 4 -> mean: 0.711 Std. Dev.: 0.047\n",
      "Set of hyperparameters 5 -> mean: 0.704 Std. Dev.: 0.053\n",
      "Set of hyperparameters 6 -> mean: 0.679 Std. Dev.: 0.038\n",
      "\n",
      "----- npv -----\n",
      "Set of hyperparameters 1 -> mean: 0.874 Std. Dev.: 0.018\n",
      "Set of hyperparameters 2 -> mean: 0.87 Std. Dev.: 0.015\n",
      "Set of hyperparameters 4 -> mean: 0.863 Std. Dev.: 0.021\n",
      "Set of hyperparameters 3 -> mean: 0.862 Std. Dev.: 0.023\n",
      "Set of hyperparameters 5 -> mean: 0.86 Std. Dev.: 0.02\n",
      "Set of hyperparameters 6 -> mean: 0.85 Std. Dev.: 0.015\n",
      "\n",
      "----- f_score -----\n",
      "Set of hyperparameters 1 -> mean: 0.736 Std. Dev.: 0.036\n",
      "Set of hyperparameters 2 -> mean: 0.715 Std. Dev.: 0.027\n",
      "Set of hyperparameters 3 -> mean: 0.701 Std. Dev.: 0.048\n",
      "Set of hyperparameters 4 -> mean: 0.695 Std. Dev.: 0.042\n",
      "Set of hyperparameters 5 -> mean: 0.682 Std. Dev.: 0.043\n",
      "Set of hyperparameters 6 -> mean: 0.663 Std. Dev.: 0.031\n",
      "\n",
      "----- err_rate -----\n",
      "Set of hyperparameters 1 -> mean: 0.258 Std. Dev.: 0.036\n",
      "Set of hyperparameters 2 -> mean: 0.274 Std. Dev.: 0.029\n",
      "Set of hyperparameters 3 -> mean: 0.288 Std. Dev.: 0.047\n",
      "Set of hyperparameters 4 -> mean: 0.291 Std. Dev.: 0.041\n",
      "Set of hyperparameters 5 -> mean: 0.301 Std. Dev.: 0.04\n",
      "Set of hyperparameters 6 -> mean: 0.319 Std. Dev.: 0.028\n"
     ]
    }
   ],
   "source": [
    "# Define an array of hyperparameter dictionaries for the kNN model\n",
    "knn_hyperparameters_array = [\n",
    "    Dict(\"n_neighbors\" => 5),\n",
    "    Dict(\"n_neighbors\" => 10),\n",
    "    Dict(\"n_neighbors\" => 15),\n",
    "    Dict(\"n_neighbors\" => 20),\n",
    "    Dict(\"n_neighbors\" => 50),\n",
    "    Dict(\"n_neighbors\" => 100) # Large neighborhoods, smooths out predictions\n",
    "]\n",
    "\n",
    "# Call the function to evaluate the model using different sets of hyperparameters and print the ranking of metrics.\n",
    "evaluateAndPrintMetricsRanking(:kNN,knn_hyperparameters_array, train_inputs, train_targets, kFoldIndices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with set of hyperparameters 1\n"
     ]
    }
   ],
   "source": [
    "# Define an array of hyperparameter dictionaries for the ANN model\n",
    "ann_hyperparameters_array = [\n",
    "    # Two-layer architecture, moderate neurons\n",
    "    Dict(\"architecture\" => [50, 30], \"activation\" => \"relu\", \"learning_rate\" => 0.01, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # One-layer architecture, fewer neurons\n",
    "    Dict(\"architecture\" => [30], \"activation\" => \"relu\", \"learning_rate\" => 0.01, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # Two-layer, different activation function\n",
    "    Dict(\"architecture\" => [50, 30], \"activation\" => \"tanh\", \"learning_rate\" => 0.01, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # One-layer, lower learning rate\n",
    "    Dict(\"architecture\" => [30], \"activation\" => \"relu\", \"learning_rate\" => 0.001, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 2000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # Two-layer, higher learning rate\n",
    "    Dict(\"architecture\" => [50, 30], \"activation\" => \"relu\", \"learning_rate\" => 0.05, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # One-layer, logistic activation\n",
    "    Dict(\"architecture\" => [30], \"activation\" => \"logistic\", \"learning_rate\" => 0.01, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # Two-layer, more neurons, different activation\n",
    "    Dict(\"architecture\" => [70, 40], \"activation\" => \"tanh\", \"learning_rate\" => 0.01, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # One-layer, more neurons\n",
    "    Dict(\"architecture\" => [50], \"activation\" => \"relu\", \"learning_rate\" => 0.01, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10)\n",
    "]\n",
    "\n",
    "# Call the function to evaluate the model using different sets of hyperparameters and print the ranking of metrics.\n",
    "evaluateAndPrintMetricsRanking(:ANN, ann_hyperparameters_array, train_inputs, train_targets, kFoldIndices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with set of hyperparameters 1\n",
      "Training with set of hyperparameters 2\n",
      "Training with set of hyperparameters 3\n",
      "Training with set of hyperparameters 4\n",
      "Training with set of hyperparameters 5\n",
      "Training with set of hyperparameters 6\n",
      "Training with set of hyperparameters 7\n",
      "Training with set of hyperparameters 8\n",
      "Training with set of hyperparameters 9\n",
      "\n",
      "----- acc -----\n",
      "Set of hyperparameters 1 -> mean: 0.722 Std. Dev.: 0.039\n",
      "Set of hyperparameters 8 -> mean: 0.714 Std. Dev.: 0.029\n",
      "Set of hyperparameters 2 -> mean: 0.695 Std. Dev.: 0.053\n",
      "Set of hyperparameters 7 -> mean: 0.684 Std. Dev.: 0.046\n",
      "Set of hyperparameters 5 -> mean: 0.682 Std. Dev.: 0.049\n",
      "Set of hyperparameters 6 -> mean: 0.682 Std. Dev.: 0.049\n",
      "Set of hyperparameters 3 -> mean: 0.665 Std. Dev.: 0.036\n",
      "Set of hyperparameters 4 -> mean: 0.647 Std. Dev.: 0.036\n",
      "Set of hyperparameters 9 -> mean: 0.405 Std. Dev.: 0.045\n",
      "\n",
      "----- sensitivity -----\n",
      "Set of hyperparameters 1 -> mean: 0.722 Std. Dev.: 0.039\n",
      "Set of hyperparameters 8 -> mean: 0.714 Std. Dev.: 0.029\n",
      "Set of hyperparameters 2 -> mean: 0.695 Std. Dev.: 0.053\n",
      "Set of hyperparameters 7 -> mean: 0.684 Std. Dev.: 0.046\n",
      "Set of hyperparameters 5 -> mean: 0.682 Std. Dev.: 0.049\n",
      "Set of hyperparameters 6 -> mean: 0.682 Std. Dev.: 0.049\n",
      "Set of hyperparameters 3 -> mean: 0.665 Std. Dev.: 0.036\n",
      "Set of hyperparameters 4 -> mean: 0.647 Std. Dev.: 0.036\n",
      "Set of hyperparameters 9 -> mean: 0.405 Std. Dev.: 0.045\n",
      "\n",
      "----- specificity -----\n",
      "Set of hyperparameters 1 -> mean: 0.86 Std. Dev.: 0.021\n",
      "Set of hyperparameters 8 -> mean: 0.857 Std. Dev.: 0.013\n",
      "Set of hyperparameters 2 -> mean: 0.847 Std. Dev.: 0.027\n",
      "Set of hyperparameters 7 -> mean: 0.842 Std. Dev.: 0.023\n",
      "Set of hyperparameters 5 -> mean: 0.842 Std. Dev.: 0.024\n",
      "Set of hyperparameters 6 -> mean: 0.842 Std. Dev.: 0.024\n",
      "Set of hyperparameters 3 -> mean: 0.832 Std. Dev.: 0.02\n",
      "Set of hyperparameters 4 -> mean: 0.825 Std. Dev.: 0.017\n",
      "Set of hyperparameters 9 -> mean: 0.702 Std. Dev.: 0.011\n",
      "\n",
      "----- ppv -----\n",
      "Set of hyperparameters 1 -> mean: 0.723 Std. Dev.: 0.046\n",
      "Set of hyperparameters 8 -> mean: 0.721 Std. Dev.: 0.026\n",
      "Set of hyperparameters 2 -> mean: 0.689 Std. Dev.: 0.059\n",
      "Set of hyperparameters 7 -> mean: 0.679 Std. Dev.: 0.049\n",
      "Set of hyperparameters 5 -> mean: 0.676 Std. Dev.: 0.053\n",
      "Set of hyperparameters 6 -> mean: 0.676 Std. Dev.: 0.053\n",
      "Set of hyperparameters 3 -> mean: 0.658 Std. Dev.: 0.042\n",
      "Set of hyperparameters 4 -> mean: 0.642 Std. Dev.: 0.037\n",
      "Set of hyperparameters 9 -> mean: 0.499 Std. Dev.: 0.064\n",
      "\n",
      "----- npv -----\n",
      "Set of hyperparameters 1 -> mean: 0.87 Std. Dev.: 0.018\n",
      "Set of hyperparameters 8 -> mean: 0.855 Std. Dev.: 0.014\n",
      "Set of hyperparameters 2 -> mean: 0.851 Std. Dev.: 0.026\n",
      "Set of hyperparameters 5 -> mean: 0.845 Std. Dev.: 0.024\n",
      "Set of hyperparameters 6 -> mean: 0.845 Std. Dev.: 0.024\n",
      "Set of hyperparameters 7 -> mean: 0.844 Std. Dev.: 0.023\n",
      "Set of hyperparameters 3 -> mean: 0.837 Std. Dev.: 0.019\n",
      "Set of hyperparameters 4 -> mean: 0.827 Std. Dev.: 0.018\n",
      "Set of hyperparameters 9 -> mean: 0.707 Std. Dev.: 0.024\n",
      "\n",
      "----- f_score -----\n",
      "Set of hyperparameters 8 -> mean: 0.714 Std. Dev.: 0.029\n",
      "Set of hyperparameters 1 -> mean: 0.709 Std. Dev.: 0.043\n",
      "Set of hyperparameters 2 -> mean: 0.686 Std. Dev.: 0.057\n",
      "Set of hyperparameters 7 -> mean: 0.678 Std. Dev.: 0.048\n",
      "Set of hyperparameters 5 -> mean: 0.673 Std. Dev.: 0.051\n",
      "Set of hyperparameters 6 -> mean: 0.673 Std. Dev.: 0.051\n",
      "Set of hyperparameters 3 -> mean: 0.656 Std. Dev.: 0.039\n",
      "Set of hyperparameters 4 -> mean: 0.639 Std. Dev.: 0.038\n",
      "Set of hyperparameters 9 -> mean: 0.354 Std. Dev.: 0.08\n",
      "\n",
      "----- err_rate -----\n",
      "Set of hyperparameters 1 -> mean: 0.278 Std. Dev.: 0.039\n",
      "Set of hyperparameters 8 -> mean: 0.286 Std. Dev.: 0.029\n",
      "Set of hyperparameters 2 -> mean: 0.305 Std. Dev.: 0.053\n",
      "Set of hyperparameters 7 -> mean: 0.316 Std. Dev.: 0.046\n",
      "Set of hyperparameters 5 -> mean: 0.318 Std. Dev.: 0.049\n",
      "Set of hyperparameters 6 -> mean: 0.318 Std. Dev.: 0.049\n",
      "Set of hyperparameters 3 -> mean: 0.335 Std. Dev.: 0.036\n",
      "Set of hyperparameters 4 -> mean: 0.353 Std. Dev.: 0.036\n",
      "Set of hyperparameters 9 -> mean: 0.595 Std. Dev.: 0.045\n"
     ]
    }
   ],
   "source": [
    "svm_hyperparameters_array = [\n",
    "    # Uses 'rbf' kernel, medium complexity with C=1.0, default polynomial degree, 'scale' for gamma \n",
    "    Dict(\"kernel\" => \"rbf\", \"degree\" => 3, \"C\" => 1.0, \"gamma\" => \"scale\"),\n",
    "    \n",
    "    # Same 'rbf' kernel, increased penalty (C=10.0) for larger-margin separation, 'auto' gamma adjusts based on features\n",
    "    Dict(\"kernel\" => \"rbf\", \"degree\" => 3, \"C\" => 10.0, \"gamma\" => \"auto\"),\n",
    "    \n",
    "    # Same 'rbf' kernel, lower penalty (C=0.1) for a softer-margin, 'scale' gamma is default scaling\n",
    "    Dict(\"kernel\" => \"rbf\", \"degree\" => 3, \"C\" => 0.1, \"gamma\" => \"scale\"),\n",
    "\n",
    "    # 'linear' kernel, suitable for less complex data\n",
    "    Dict(\"kernel\" => \"linear\", \"degree\" => 3, \"C\" => 0.1, \"gamma\" => \"auto\"),\n",
    "    \n",
    "    # 'linear' kernel, with C=1.0 indicating a balance between margin and misclassification\n",
    "    Dict(\"kernel\" => \"linear\", \"degree\" => 3, \"C\" => 1.0, \"gamma\" => \"auto\"),\n",
    "\n",
    "    # 'linear' kernel, with a medium penalty and scale gamma\n",
    "    Dict(\"kernel\" => \"linear\", \"degree\" => 3, \"C\" => 1.0, \"gamma\" => \"scale\"),\n",
    "\n",
    "    # 'linear' kernel with a higher penalty and scale gamma\n",
    "    Dict(\"kernel\" => \"linear\", \"degree\" => 3, \"C\" => 10.0, \"gamma\" => \"scale\"),\n",
    "    \n",
    "    # 'poly' kernel, polynomial degree is set twice by mistake, should only be 'degree' => 3, 'scale' gamma defaults to feature scale\n",
    "    Dict(\"kernel\" => \"poly\", \"degree\" => 3, \"C\" => 1.0, \"gamma\" => \"scale\"),\n",
    "    \n",
    "    # 'poly' kernel, increased polynomial degree (5) for higher model complexity, 'auto' gamma may overfit with high dimension\n",
    "    Dict(\"kernel\" => \"poly\", \"degree\" => 5, \"C\" => 1.0, \"gamma\" => \"auto\")\n",
    "]\n",
    "\n",
    "\n",
    "# Call the function to evaluate the model using different sets of hyperparameters and print the ranking of metrics.\n",
    "evaluateAndPrintMetricsRanking(:SVM, svm_hyperparameters_array, train_inputs, train_targets, kFoldIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\n",
    "using Plots\n",
    "\n",
    "# Define the data for each model\n",
    "ann_means = [0.947, 0.947, 0.925, 0.788, 0.948, 0.933, 0.8, 0.9]\n",
    "ann_stds = [0.018, 0.04, 0.07, 0.097, 0.036, 0.039, 0.4, 0.3]\n",
    "svm_means = [0.947, 0.947, 0.927, 0.953, 0.94, 0.4, 0.5]\n",
    "svm_stds = [0.03, 0.038, 0.092, 0.051, 0.043, 0.082, 0.7]\n",
    "dt_means = [0.927, 0.913, 0.913, 0.913, 0.913, 0.913]\n",
    "dt_stds = [0.043, 0.045, 0.045, 0.045, 0.045, 0.045]\n",
    "knn_means = [0.947, 0.947, 0.96, 0.94, 0.913, 0.507]\n",
    "knn_stds = [0.038, 0.038, 0.015, 0.043, 0.104, 0.068]\n",
    "\n",
    "# Create subplots for each model\n",
    "p1 = bar(1:6, ann_means, yerr=ann_stds, title=\"ANN\", legend=false)\n",
    "p2 = bar(1:6, svm_means, yerr=svm_stds, title=\"SVM\", legend=false)\n",
    "p3 = bar(1:6, dt_means, yerr=dt_stds, title=\"Decision Tree\", legend=false)\n",
    "p4 = bar(1:6, knn_means, yerr=knn_stds, title=\"KNN\", legend=false)\n",
    "\n",
    "# Customize the y-axis and labels\n",
    "for p in [p1, p2, p3, p4]\n",
    "    ylabel!(p, \"Accuracy\")\n",
    "    xlabel!(p, \"Set of Hyperparameters\")\n",
    "end\n",
    "\n",
    "# Combine the plots into one figure\n",
    "plot(p1, p2, p3, p4, layout=(2,2), size=(800,600))\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree model accuracy: 33.36083953717822 %\n"
     ]
    }
   ],
   "source": [
    "using MLBase\n",
    "using JLD\n",
    "\n",
    "dt_model = DecisionTreeClassifier(max_depth=10)\n",
    "\n",
    "# Fit the model on the training data\n",
    "fit!(dt_model, train_inputs, train_targets)\n",
    "\n",
    "# Predict the targets for the test data\n",
    "predicted_targets = predict(dt_model, test_inputs)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "dt_model_accuracy = mean(predicted_targets .== test_targets)\n",
    "println(\"Decision Tree model accuracy: $(dt_model_accuracy * 100) %\")\n",
    "\n",
    "# Save the model\n",
    "#JLD.save(\"dt_model.jld\", \"model\", dt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN model accuracy: 33.303435285675846 %\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# Fit the model on the training data\n",
    "fit!(knn_model, train_inputs, train_targets)\n",
    "\n",
    "# Predict the targets for the test data\n",
    "predicted_targets = predict(knn_model, test_inputs)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "knn_model_accuracy = mean(predicted_targets .== test_targets)\n",
    "println(\"KNN model accuracy: $(knn_model_accuracy * 100) %\")\n",
    "\n",
    "# Save the model\n",
    "#JLD.save(\"knn_model.jld\", \"model\", knn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN model accuracy: 33.31958023141089 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\coros\\.julia\\conda\\3\\x86_64\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ann_model = MLPClassifier(hidden_layer_sizes=(70, 40), activation=\"tanh\", learning_rate_init=0.01, validation_fraction=0.1, n_iter_no_change=80, max_iter=1000)\n",
    "\n",
    "# Fit the model on the training data\n",
    "fit!(ann_model, train_inputs, train_targets)\n",
    "\n",
    "# Predict the targets for the test data\n",
    "predicted_targets = predict(ann_model, test_inputs)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "ann_model_accuracy = mean(predicted_targets .== test_targets)\n",
    "println(\"ANN model accuracy: $(ann_model_accuracy * 100) %\")\n",
    "\n",
    "# Save the model\n",
    "#JLD.save(\"ann_model.jld\", \"model\", ann_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model accuracy: 34.38335276706431 %\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(kernel=\"rbf\", degree=3, C=1.0, gamma=\"scale\")\n",
    "\n",
    "# Fit the model on the training data\n",
    "fit!(svm_model, train_inputs, train_targets)\n",
    "\n",
    "# Predict the targets for the test data\n",
    "predicted_targets = predict(svm_model, test_inputs)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "knn_model_accuracy = mean(predicted_targets .== test_targets)\n",
    "println(\"SVM model accuracy: $(knn_model_accuracy * 100) %\")\n",
    "\n",
    "# Save the model\n",
    "#JLD.save(\"svm_model.jld\", \"model\", svm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble model accuracy: 34.10888868956857 %\n"
     ]
    }
   ],
   "source": [
    "# Define the base models with the chosen hyperparameters\n",
    "dt_model = DecisionTreeClassifier(max_depth=10)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=10)\n",
    "ann_model = MLPClassifier(hidden_layer_sizes=(70, 40), activation=\"tanh\", learning_rate_init=0.01, validation_fraction=0.1, n_iter_no_change=80, max_iter=10000) # Increase max_iter from 1000 to ensure convergence\n",
    "svm_model = SVC(kernel=\"rbf\", degree=3, C=1.0, gamma=\"scale\")\n",
    "\n",
    "# Create a list of tuples (name, model) for the base models\n",
    "base_models = [\n",
    "    (\"DecisionTree\", dt_model),\n",
    "    (\"kNN\", knn_model),\n",
    "    (\"ANN\", ann_model),\n",
    "    (\"SVM\", svm_model)\n",
    "]\n",
    "\n",
    "# Choose a final estimator for the stacking ensemble\n",
    "# Logistic Regression is a common choice for combining predictions\n",
    "final_estimator = LogisticRegression()\n",
    "\n",
    "# Create the stacking ensemble\n",
    "ensemble = StackingClassifier(estimators=base_models, final_estimator=final_estimator)\n",
    "\n",
    "# Train the ensemble model\n",
    "fit!(ensemble, train_inputs, train_targets)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "model_accuracy = score(ensemble, test_inputs, test_targets)\n",
    "println(\"Ensemble model accuracy: $(model_accuracy * 100) %\")\n",
    "\n",
    "# Save the model\n",
    "#JLD.save(\"ensemble.jld\", \"model\", ensemble)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy-pasted from unit 6 but something similar would go here ##\n",
    "\n",
    "### Best model configuration\n",
    "\n",
    "Based on the hyperparameters provided for each model and the results we have, the best configurations for each model according to **accuracy** are:\n",
    "\n",
    "\n",
    "- **Artificial Neural Network (ANN)**: The best-performing ANN model uses the architecture [100, 100, 100] with 'relu' activation, a learning rate of 0.01, a validation ratio of 0.1, and a maximum of 1000 iterations. This suggests that a more complex model with a higher number of neurons was able to capture the complexity of the data better than simpler models.\n",
    "\n",
    "- **Support Vector Machine (SVM)**: The SVM model that performed best had the 'linear' kernel with a C value of 1.0 and 'auto' gamma setting. This indicates a model that balances margin and misclassification error, without the need for the complexity of a non-linear kernel.\n",
    "\n",
    "- **Decision Tree**: The best decision tree model had a maximum depth of 3. This suggests that a simpler model, which avoids overfitting by not going too deep into the tree, was sufficient to capture the relevant patterns in the data.\n",
    "\n",
    "- **K-Nearest Neighbors (KNN)**: The KNN model that yielded the best results had 15 neighbors. This points to an intermediate complexity that balances between smoothing out the noise and capturing sufficient detail from the dataset.\n",
    "\n",
    "We shall see their performance in the following table:\n",
    "\n",
    "\n",
    "| Best Model Configuration          | Accuracy         | Sensitivity      | Specificity      | PPV              | NPV              | F_Score          | Err_Rate         |\n",
    "|---------------------|------------------|------------------|------------------|------------------|------------------|------------------|------------------|\n",
    "| ANN (Model 5)       | 0.948 ± 0.036    | 0.948 ± 0.036    | 0.979 ± 0.013    | 0.959 ± 0.027    | 0.969 ± 0.025    | 0.948 ± 0.036    | 0.052 ± 0.036    |\n",
    "| SVM (Model 4)       | 0.953 ± 0.051    | 0.953 ± 0.051    | 0.980 ± 0.022    | 0.964 ± 0.034    | 0.973 ± 0.033    | 0.953 ± 0.051    | 0.047 ± 0.051    |\n",
    "| Decision Tree (Model 1) | 0.927 ± 0.043 | 0.927 ± 0.043    | 0.967 ± 0.021    | 0.940 ± 0.032    | 0.955 ± 0.032    | 0.927 ± 0.043    | 0.073 ± 0.043    |\n",
    "| KNN (Model 3)       | 0.96 ± 0.015     | 0.96 ± 0.015     | 0.98 ± 0.009     | 0.965 ± 0.011    | 0.978 ± 0.015    | 0.96 ± 0.015     | 0.04 ± 0.015     |\n",
    "\n",
    "### Best performing model \n",
    "\n",
    "The KNN model outperforms the others with the highest accuracy of 0.96 and the lowest standard deviation (0.015). It appears to offer the best compromise between bias and variance, which is a key factor in its superior performance in this evaluation. It's worth noting, though, that the ideal model could vary depending on the specific data and the context of the problem. While accuracy is a critical metric, and we believe is a good metric for our use case, it may not be the sole criterion for success in other cases, especially when the consequences of false positives and false negatives differ significantly. In such scenarios, other metrics like sensitivity or specificity may be more relevant for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
