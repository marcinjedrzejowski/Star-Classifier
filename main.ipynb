{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluateAndPrintMetricsRanking (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"utils/preprocessing.jl\")\n",
    "include(\"utils/model_evaluation.jl\")\n",
    "include(\"utils/data_loader.jl\")\n",
    "include(\"utils/visualization.jl\")\n",
    "include(\"utils/ml1_utils.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "Random.seed!(123)\n",
    "\n",
    "data = DataLoader.load_data_for_analysis(\"dataset\\\\star_classification.csv\");\n",
    "#Visualization.entry_visualization(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I've changed the preprocess_data function so it doesn't OneHotEncode the targets\\n    because it's not needed&advised for KNN, DT & SVM, only for ANN. \\n    The OneHotEncoding for the ANN will be done in the modelCrossValidation function,\\n    which is called by the evaluateAndPrintMetricsRanking function.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DataFrames\n",
    "\n",
    "data = DataLoader.load_data(\"dataset\\\\star_classification.csv\");\n",
    "\n",
    "# Preprocess the data\n",
    "\n",
    "\"\"\"    This function does the following:\n",
    "        - Balance the data using the undersampling method\n",
    "        - Parse the data: chosing the correct columns for inputs and targets\n",
    "        - Convert the input into an 2D array of floats\n",
    "        - Normalize the inputs  ------> this should be done after the train-test split!!!!\n",
    "\"\"\"\n",
    "\n",
    "inputs, targets = Preprocessing.preprocess_data(data)\n",
    "\n",
    "\"\"\" I've changed the preprocess_data function so it doesn't OneHotEncode the targets\n",
    "    because it's not needed&advised for KNN, DT & SVM, only for ANN. \n",
    "    The OneHotEncoding for the ANN will be done in the modelCrossValidation function,\n",
    "    which is called by the evaluateAndPrintMetricsRanking function.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix{Float32}Vector{Any}\n",
      "(56883, 5)(56883,)\n",
      "Any[\"GALAXY\", \"GALAXY\", \"GALAXY\", \"GALAXY\", \"GALAXY\", \"GALAXY\", \"GALAXY\", \"GALAXY\", \"GALAXY\", \"GALAXY\"]\n"
     ]
    }
   ],
   "source": [
    "println(typeof(inputs), typeof(targets))\n",
    "println(size(inputs),size(targets))\n",
    "println(targets[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inputs: (39818, 5)\n",
      "Train targets: (39818,)\n",
      "Test inputs: (17065, 5)\n",
      "Test targets: (17065,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initial holdOut split of the data\"\"\"\n",
    "\n",
    "N = size(inputs, 1)\n",
    "\n",
    "# Split to train and test using the holdOut function\n",
    "train_indices, test_indices = holdOut(N, 0.98)\n",
    "\n",
    "# Extract training and testing data\n",
    "train_inputs = inputs[train_indices, :]\n",
    "train_targets = targets[train_indices]\n",
    "test_inputs = inputs[test_indices, :]\n",
    "test_targets = targets[test_indices]\n",
    "\n",
    "# Check size of train and test sets\n",
    "println(\"Train inputs: \", size(train_inputs))\n",
    "println(\"Train targets: \", size(train_targets))\n",
    "println(\"Test inputs: \", size(test_inputs))\n",
    "println(\"Test targets: \", size(test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing hyperparameters for each model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.neighbors._classification.KNeighborsClassifier'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "\n",
    "@sk_import neural_network: MLPClassifier\n",
    "@sk_import svm: SVC\n",
    "@sk_import tree: DecisionTreeClassifier\n",
    "@sk_import neighbors: KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setting indices for the k-fold cross-validation\n",
    "    we are about to do with the different models\n",
    "\"\"\"\n",
    "N=size(train_inputs,1)\n",
    "k = 5 # number of folds\n",
    "kFoldIndices = crossvalidation(N, k);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with set of hyperparameters 1\n",
      "Training with set of hyperparameters 2\n",
      "Training with set of hyperparameters 3\n",
      "Training with set of hyperparameters 4\n",
      "Training with set of hyperparameters 5\n",
      "Training with set of hyperparameters 6\n",
      "\n",
      "----- acc -----\n",
      "Set of hyperparameters 3 -> mean: 0.782 Std. Dev.: 0.003\n",
      "Set of hyperparameters 4 -> mean: 0.774 Std. Dev.: 0.003\n",
      "Set of hyperparameters 5 -> mean: 0.764 Std. Dev.: 0.004\n",
      "Set of hyperparameters 6 -> mean: 0.764 Std. Dev.: 0.004\n",
      "Set of hyperparameters 2 -> mean: 0.697 Std. Dev.: 0.006\n",
      "Set of hyperparameters 1 -> mean: 0.634 Std. Dev.: 0.004\n",
      "\n",
      "----- sensitivity -----\n",
      "Set of hyperparameters 3 -> mean: 0.782 Std. Dev.: 0.003\n",
      "Set of hyperparameters 4 -> mean: 0.774 Std. Dev.: 0.003\n",
      "Set of hyperparameters 5 -> mean: 0.764 Std. Dev.: 0.004\n",
      "Set of hyperparameters 6 -> mean: 0.764 Std. Dev.: 0.004\n",
      "Set of hyperparameters 2 -> mean: 0.697 Std. Dev.: 0.006\n",
      "Set of hyperparameters 1 -> mean: 0.634 Std. Dev.: 0.004\n",
      "\n",
      "----- specificity -----\n",
      "Set of hyperparameters 3 -> mean: 0.891 Std. Dev.: 0.001\n",
      "Set of hyperparameters 4 -> mean: 0.887 Std. Dev.: 0.002\n",
      "Set of hyperparameters 5 -> mean: 0.882 Std. Dev.: 0.002\n",
      "Set of hyperparameters 6 -> mean: 0.882 Std. Dev.: 0.002\n",
      "Set of hyperparameters 2 -> mean: 0.849 Std. Dev.: 0.003\n",
      "Set of hyperparameters 1 -> mean: 0.817 Std. Dev.: 0.002\n",
      "\n",
      "----- ppv -----\n",
      "Set of hyperparameters 3 -> mean: 0.782 Std. Dev.: 0.002\n",
      "Set of hyperparameters 4 -> mean: 0.774 Std. Dev.: 0.003\n",
      "Set of hyperparameters 5 -> mean: 0.764 Std. Dev.: 0.004\n",
      "Set of hyperparameters 6 -> mean: 0.764 Std. Dev.: 0.004\n",
      "Set of hyperparameters 2 -> mean: 0.695 Std. Dev.: 0.006\n",
      "Set of hyperparameters 1 -> mean: 0.636 Std. Dev.: 0.007\n",
      "\n",
      "----- npv -----\n",
      "Set of hyperparameters 3 -> mean: 0.892 Std. Dev.: 0.001\n",
      "Set of hyperparameters 4 -> mean: 0.887 Std. Dev.: 0.002\n",
      "Set of hyperparameters 5 -> mean: 0.882 Std. Dev.: 0.002\n",
      "Set of hyperparameters 6 -> mean: 0.882 Std. Dev.: 0.002\n",
      "Set of hyperparameters 2 -> mean: 0.851 Std. Dev.: 0.002\n",
      "Set of hyperparameters 1 -> mean: 0.823 Std. Dev.: 0.005\n",
      "\n",
      "----- f_score -----\n",
      "Set of hyperparameters 3 -> mean: 0.782 Std. Dev.: 0.003\n",
      "Set of hyperparameters 4 -> mean: 0.774 Std. Dev.: 0.003\n",
      "Set of hyperparameters 5 -> mean: 0.764 Std. Dev.: 0.004\n",
      "Set of hyperparameters 6 -> mean: 0.764 Std. Dev.: 0.004\n",
      "Set of hyperparameters 2 -> mean: 0.694 Std. Dev.: 0.007\n",
      "Set of hyperparameters 1 -> mean: 0.627 Std. Dev.: 0.006\n",
      "\n",
      "----- err_rate -----\n",
      "Set of hyperparameters 3 -> mean: 0.218 Std. Dev.: 0.003\n",
      "Set of hyperparameters 4 -> mean: 0.226 Std. Dev.: 0.003\n",
      "Set of hyperparameters 5 -> mean: 0.236 Std. Dev.: 0.004\n",
      "Set of hyperparameters 6 -> mean: 0.236 Std. Dev.: 0.004\n",
      "Set of hyperparameters 2 -> mean: 0.303 Std. Dev.: 0.006\n",
      "Set of hyperparameters 1 -> mean: 0.366 Std. Dev.: 0.004\n"
     ]
    }
   ],
   "source": [
    "# Define an array of hyperparameter dictionaries for the Decision Tree model\n",
    "dtree_hyperparameters_array = [\n",
    "    Dict(\"max_depth\" => 3),\n",
    "    Dict(\"max_depth\" => 5),\n",
    "    Dict(\"max_depth\" => 10),\n",
    "    Dict(\"max_depth\" => 20),\n",
    "    Dict(\"max_depth\" => 50),\n",
    "    Dict(\"max_depth\" => 100) # Deeper trees can capture more detail but risk overfitting\n",
    "]\n",
    "\n",
    "# Call the function to evaluate the model using different sets of hyperparameters and print the ranking of metrics.\n",
    "evaluateAndPrintMetricsRanking(:DecisionTree,dtree_hyperparameters_array, train_inputs, train_targets, kFoldIndices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with set of hyperparameters 1\n",
      "Training with set of hyperparameters 2\n",
      "Training with set of hyperparameters 3\n",
      "Training with set of hyperparameters 4\n",
      "Training with set of hyperparameters 5\n",
      "Training with set of hyperparameters 6\n",
      "\n",
      "----- acc -----\n",
      "Set of hyperparameters 2 -> mean: 0.822 Std. Dev.: 0.004\n",
      "Set of hyperparameters 3 -> mean: 0.822 Std. Dev.: 0.003\n",
      "Set of hyperparameters 1 -> mean: 0.822 Std. Dev.: 0.002\n",
      "Set of hyperparameters 4 -> mean: 0.819 Std. Dev.: 0.003\n",
      "Set of hyperparameters 5 -> mean: 0.805 Std. Dev.: 0.005\n",
      "Set of hyperparameters 6 -> mean: 0.787 Std. Dev.: 0.005\n",
      "\n",
      "----- sensitivity -----\n",
      "Set of hyperparameters 2 -> mean: 0.822 Std. Dev.: 0.004\n",
      "Set of hyperparameters 3 -> mean: 0.822 Std. Dev.: 0.003\n",
      "Set of hyperparameters 1 -> mean: 0.822 Std. Dev.: 0.002\n",
      "Set of hyperparameters 4 -> mean: 0.819 Std. Dev.: 0.003\n",
      "Set of hyperparameters 5 -> mean: 0.805 Std. Dev.: 0.005\n",
      "Set of hyperparameters 6 -> mean: 0.787 Std. Dev.: 0.005\n",
      "\n",
      "----- specificity -----\n",
      "Set of hyperparameters 2 -> mean: 0.911 Std. Dev.: 0.002\n",
      "Set of hyperparameters 3 -> mean: 0.911 Std. Dev.: 0.001\n",
      "Set of hyperparameters 1 -> mean: 0.911 Std. Dev.: 0.001\n",
      "Set of hyperparameters 4 -> mean: 0.909 Std. Dev.: 0.002\n",
      "Set of hyperparameters 5 -> mean: 0.903 Std. Dev.: 0.002\n",
      "Set of hyperparameters 6 -> mean: 0.893 Std. Dev.: 0.003\n",
      "\n",
      "----- ppv -----\n",
      "Set of hyperparameters 2 -> mean: 0.823 Std. Dev.: 0.004\n",
      "Set of hyperparameters 3 -> mean: 0.822 Std. Dev.: 0.003\n",
      "Set of hyperparameters 1 -> mean: 0.822 Std. Dev.: 0.001\n",
      "Set of hyperparameters 4 -> mean: 0.819 Std. Dev.: 0.003\n",
      "Set of hyperparameters 5 -> mean: 0.806 Std. Dev.: 0.004\n",
      "Set of hyperparameters 6 -> mean: 0.787 Std. Dev.: 0.005\n",
      "\n",
      "----- npv -----\n",
      "Set of hyperparameters 2 -> mean: 0.912 Std. Dev.: 0.002\n",
      "Set of hyperparameters 3 -> mean: 0.912 Std. Dev.: 0.002\n",
      "Set of hyperparameters 1 -> mean: 0.911 Std. Dev.: 0.001\n",
      "Set of hyperparameters 4 -> mean: 0.91 Std. Dev.: 0.002\n",
      "Set of hyperparameters 5 -> mean: 0.904 Std. Dev.: 0.002\n",
      "Set of hyperparameters 6 -> mean: 0.895 Std. Dev.: 0.003\n",
      "\n",
      "----- f_score -----\n",
      "Set of hyperparameters 2 -> mean: 0.821 Std. Dev.: 0.004\n",
      "Set of hyperparameters 3 -> mean: 0.821 Std. Dev.: 0.003\n",
      "Set of hyperparameters 1 -> mean: 0.821 Std. Dev.: 0.002\n",
      "Set of hyperparameters 4 -> mean: 0.818 Std. Dev.: 0.003\n",
      "Set of hyperparameters 5 -> mean: 0.804 Std. Dev.: 0.005\n",
      "Set of hyperparameters 6 -> mean: 0.785 Std. Dev.: 0.006\n",
      "\n",
      "----- err_rate -----\n",
      "Set of hyperparameters 2 -> mean: 0.178 Std. Dev.: 0.004\n",
      "Set of hyperparameters 3 -> mean: 0.178 Std. Dev.: 0.003\n",
      "Set of hyperparameters 1 -> mean: 0.178 Std. Dev.: 0.002\n",
      "Set of hyperparameters 4 -> mean: 0.181 Std. Dev.: 0.003\n",
      "Set of hyperparameters 5 -> mean: 0.195 Std. Dev.: 0.005\n",
      "Set of hyperparameters 6 -> mean: 0.213 Std. Dev.: 0.005\n"
     ]
    }
   ],
   "source": [
    "# Define an array of hyperparameter dictionaries for the kNN model\n",
    "knn_hyperparameters_array = [\n",
    "    Dict(\"n_neighbors\" => 5),\n",
    "    Dict(\"n_neighbors\" => 10),\n",
    "    Dict(\"n_neighbors\" => 15),\n",
    "    Dict(\"n_neighbors\" => 20),\n",
    "    Dict(\"n_neighbors\" => 50),\n",
    "    Dict(\"n_neighbors\" => 100) # Large neighborhoods, smooths out predictions\n",
    "]\n",
    "\n",
    "# Call the function to evaluate the model using different sets of hyperparameters and print the ranking of metrics.\n",
    "evaluateAndPrintMetricsRanking(:kNN,knn_hyperparameters_array, train_inputs, train_targets, kFoldIndices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with set of hyperparameters 1\n",
      "Training with set of hyperparameters 2\n",
      "Training with set of hyperparameters 3\n",
      "Training with set of hyperparameters 4\n",
      "Training with set of hyperparameters 5\n",
      "Training with set of hyperparameters 6\n",
      "Training with set of hyperparameters 7"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\coros\\.julia\\conda\\3\\x86_64\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\coros\\.julia\\conda\\3\\x86_64\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\coros\\.julia\\conda\\3\\x86_64\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\coros\\.julia\\conda\\3\\x86_64\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\coros\\.julia\\conda\\3\\x86_64\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\coros\\.julia\\conda\\3\\x86_64\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with set of hyperparameters 8\n",
      "\n",
      "----- acc -----\n",
      "Set of hyperparameters 7 -> mean: 0.833 Std. Dev.: 0.003\n",
      "Set of hyperparameters 1 -> mean: 0.832 Std. Dev.: 0.003\n",
      "Set of hyperparameters 3 -> mean: 0.831 Std. Dev.: 0.001\n",
      "Set of hyperparameters 5 -> mean: 0.819 Std. Dev.: 0.002\n",
      "Set of hyperparameters 8 -> mean: 0.814 Std. Dev.: 0.004\n",
      "Set of hyperparameters 2 -> mean: 0.807 Std. Dev.: 0.003\n",
      "Set of hyperparameters 6 -> mean: 0.803 Std. Dev.: 0.004\n",
      "Set of hyperparameters 4 -> mean: 0.796 Std. Dev.: 0.006\n",
      "\n",
      "----- sensitivity -----\n",
      "Set of hyperparameters 7 -> mean: 0.833 Std. Dev.: 0.003\n",
      "Set of hyperparameters 1 -> mean: 0.832 Std. Dev.: 0.003\n",
      "Set of hyperparameters 3 -> mean: 0.831 Std. Dev.: 0.001\n",
      "Set of hyperparameters 5 -> mean: 0.819 Std. Dev.: 0.002\n",
      "Set of hyperparameters 8 -> mean: 0.814 Std. Dev.: 0.004\n",
      "Set of hyperparameters 2 -> mean: 0.807 Std. Dev.: 0.003\n",
      "Set of hyperparameters 6 -> mean: 0.803 Std. Dev.: 0.004\n",
      "Set of hyperparameters 4 -> mean: 0.796 Std. Dev.: 0.006\n",
      "\n",
      "----- specificity -----\n",
      "Set of hyperparameters 7 -> mean: 0.917 Std. Dev.: 0.002\n",
      "Set of hyperparameters 1 -> mean: 0.916 Std. Dev.: 0.001\n",
      "Set of hyperparameters 3 -> mean: 0.916 Std. Dev.: 0.001\n",
      "Set of hyperparameters 5 -> mean: 0.91 Std. Dev.: 0.001\n",
      "Set of hyperparameters 8 -> mean: 0.907 Std. Dev.: 0.002\n",
      "Set of hyperparameters 2 -> mean: 0.903 Std. Dev.: 0.001\n",
      "Set of hyperparameters 6 -> mean: 0.902 Std. Dev.: 0.002\n",
      "Set of hyperparameters 4 -> mean: 0.898 Std. Dev.: 0.003\n",
      "\n",
      "----- ppv -----\n",
      "Set of hyperparameters 7 -> mean: 0.834 Std. Dev.: 0.003\n",
      "Set of hyperparameters 1 -> mean: 0.833 Std. Dev.: 0.003\n",
      "Set of hyperparameters 3 -> mean: 0.832 Std. Dev.: 0.001\n",
      "Set of hyperparameters 5 -> mean: 0.821 Std. Dev.: 0.002\n",
      "Set of hyperparameters 8 -> mean: 0.815 Std. Dev.: 0.004\n",
      "Set of hyperparameters 2 -> mean: 0.808 Std. Dev.: 0.002\n",
      "Set of hyperparameters 6 -> mean: 0.804 Std. Dev.: 0.004\n",
      "Set of hyperparameters 4 -> mean: 0.797 Std. Dev.: 0.006\n",
      "\n",
      "----- npv -----\n",
      "Set of hyperparameters 7 -> mean: 0.917 Std. Dev.: 0.002\n",
      "Set of hyperparameters 1 -> mean: 0.917 Std. Dev.: 0.001\n",
      "Set of hyperparameters 3 -> mean: 0.916 Std. Dev.: 0.001\n",
      "Set of hyperparameters 5 -> mean: 0.911 Std. Dev.: 0.001\n",
      "Set of hyperparameters 8 -> mean: 0.908 Std. Dev.: 0.002\n",
      "Set of hyperparameters 2 -> mean: 0.904 Std. Dev.: 0.001\n",
      "Set of hyperparameters 6 -> mean: 0.903 Std. Dev.: 0.002\n",
      "Set of hyperparameters 4 -> mean: 0.899 Std. Dev.: 0.003\n",
      "\n",
      "----- f_score -----\n",
      "Set of hyperparameters 7 -> mean: 0.833 Std. Dev.: 0.003\n",
      "Set of hyperparameters 1 -> mean: 0.831 Std. Dev.: 0.003\n",
      "Set of hyperparameters 3 -> mean: 0.831 Std. Dev.: 0.001\n",
      "Set of hyperparameters 5 -> mean: 0.818 Std. Dev.: 0.002\n",
      "Set of hyperparameters 8 -> mean: 0.813 Std. Dev.: 0.004\n",
      "Set of hyperparameters 2 -> mean: 0.805 Std. Dev.: 0.003\n",
      "Set of hyperparameters 6 -> mean: 0.802 Std. Dev.: 0.004\n",
      "Set of hyperparameters 4 -> mean: 0.795 Std. Dev.: 0.006\n",
      "\n",
      "----- err_rate -----\n",
      "Set of hyperparameters 7 -> mean: 0.167 Std. Dev.: 0.003\n",
      "Set of hyperparameters 1 -> mean: 0.168 Std. Dev.: 0.003\n",
      "Set of hyperparameters 3 -> mean: 0.169 Std. Dev.: 0.001\n",
      "Set of hyperparameters 5 -> mean: 0.181 Std. Dev.: 0.002\n",
      "Set of hyperparameters 8 -> mean: 0.186 Std. Dev.: 0.004\n",
      "Set of hyperparameters 2 -> mean: 0.193 Std. Dev.: 0.003\n",
      "Set of hyperparameters 6 -> mean: 0.197 Std. Dev.: 0.004\n",
      "Set of hyperparameters 4 -> mean: 0.204 Std. Dev.: 0.006\n"
     ]
    }
   ],
   "source": [
    "# Define an array of hyperparameter dictionaries for the ANN model\n",
    "ann_hyperparameters_array = [\n",
    "    # Two-layer architecture, moderate neurons\n",
    "    Dict(\"architecture\" => [50, 30], \"activation\" => \"relu\", \"learning_rate\" => 0.01, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # One-layer architecture, fewer neurons\n",
    "    Dict(\"architecture\" => [30], \"activation\" => \"relu\", \"learning_rate\" => 0.01, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # Two-layer, different activation function\n",
    "    Dict(\"architecture\" => [50, 30], \"activation\" => \"tanh\", \"learning_rate\" => 0.01, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # One-layer, lower learning rate\n",
    "    Dict(\"architecture\" => [30], \"activation\" => \"relu\", \"learning_rate\" => 0.001, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 2000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # Two-layer, higher learning rate\n",
    "    Dict(\"architecture\" => [50, 30], \"activation\" => \"relu\", \"learning_rate\" => 0.05, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # One-layer, logistic activation\n",
    "    Dict(\"architecture\" => [30], \"activation\" => \"logistic\", \"learning_rate\" => 0.01, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # Two-layer, more neurons, different activation\n",
    "    Dict(\"architecture\" => [70, 40], \"activation\" => \"tanh\", \"learning_rate\" => 0.01, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # One-layer, more neurons\n",
    "    Dict(\"architecture\" => [50], \"activation\" => \"relu\", \"learning_rate\" => 0.01, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10)\n",
    "]\n",
    "\n",
    "# Call the function to evaluate the model using different sets of hyperparameters and print the ranking of metrics.\n",
    "evaluateAndPrintMetricsRanking(:ANN, ann_hyperparameters_array, train_inputs, train_targets, kFoldIndices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with set of hyperparameters 1\n",
      "Training with set of hyperparameters 2\n",
      "Training with set of hyperparameters 3\n",
      "Training with set of hyperparameters 4\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "KeyError: key \"degree\" not found",
     "output_type": "error",
     "traceback": [
      "KeyError: key \"degree\" not found",
      "",
      "Stacktrace:",
      " [1] getindex",
      "   @ .\\dict.jl:484 [inlined]",
      " [2] modelCrossValidation(modelType::Symbol, modelHyperparameters::Dict{String, Any}, inputs::Matrix{Float32}, targets::Vector{Any}, crossValidationIndices::Vector{Int64})",
      "   @ Main c:\\Users\\coros\\2023-24\\ML1\\Star-Classifier\\utils\\ml1_utils.jl:742",
      " [3] collectMetrics(model::Symbol, hyperparameters_array::Vector{Dict{String, Any}}, norm_inputs::Matrix{Float32}, targets::Vector{Any}, kFoldIndices::Vector{Int64})",
      "   @ Main c:\\Users\\coros\\2023-24\\ML1\\Star-Classifier\\utils\\ml1_utils.jl:837",
      " [4] evaluateAndPrintMetricsRanking(model::Symbol, hyperparameters_array::Vector{Dict{String, Any}}, norm_inputs::Matrix{Float32}, targets::Vector{Any}, kFoldIndices::Vector{Int64})",
      "   @ Main c:\\Users\\coros\\2023-24\\ML1\\Star-Classifier\\utils\\ml1_utils.jl:869",
      " [5] top-level scope",
      "   @ In[12]:29"
     ]
    }
   ],
   "source": [
    "svm_hyperparameters_array = [\n",
    "    # Uses 'rbf' kernel, medium complexity with C=1.0, default polynomial degree, 'scale' for gamma \n",
    "    Dict(\"kernel\" => \"rbf\", \"degree\" => 3, \"C\" => 1.0, \"gamma\" => \"scale\"),\n",
    "    \n",
    "    # Same 'rbf' kernel, increased penalty (C=10.0) for larger-margin separation, 'auto' gamma adjusts based on features\n",
    "    Dict(\"kernel\" => \"rbf\", \"degree\" => 3, \"C\" => 10.0, \"gamma\" => \"auto\"),\n",
    "    \n",
    "    # Same 'rbf' kernel, lower penalty (C=0.1) for a softer-margin, 'scale' gamma is default scaling\n",
    "    Dict(\"kernel\" => \"rbf\", \"degree\" => 3, \"C\" => 0.1, \"gamma\" => \"scale\"),\n",
    "\n",
    "    # 'linear' kernel, suitable for less complex data\n",
    "    Dict(\"kernel\" => \"linear\", \"C\" => 0.1),\n",
    "    \n",
    "    # 'linear' kernel, not affected by 'degree' or 'gamma', with C=1.0 indicating a balance between margin and misclassification\n",
    "    Dict(\"kernel\" => \"linear\", \"degree\" => 3, \"C\" => 1.0, \"gamma\" => \"auto\"),\n",
    "\n",
    "    # 'linear' kernel with a higher penalty, stricter margin\n",
    "    Dict(\"kernel\" => \"linear\", \"C\" => 10.0),\n",
    "    \n",
    "    # 'poly' kernel, polynomial degree is set twice by mistake, should only be 'degree' => 3, 'scale' gamma defaults to feature scale\n",
    "    Dict(\"kernel\" => \"poly\", \"degree\" => 3, \"C\" => 1.0, \"gamma\" => \"scale\"),\n",
    "    \n",
    "    # 'poly' kernel, increased polynomial degree (5) for higher model complexity, 'auto' gamma may overfit with high dimension\n",
    "    Dict(\"kernel\" => \"poly\", \"degree\" => 5, \"C\" => 1.0, \"gamma\" => \"auto\")\n",
    "]\n",
    "\n",
    "\n",
    "# Call the function to evaluate the model using different sets of hyperparameters and print the ranking of metrics.\n",
    "evaluateAndPrintMetricsRanking(:SVM, svm_hyperparameters_array, train_inputs, train_targets, kFoldIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\n",
    "using Plots\n",
    "\n",
    "# Define the data for each model\n",
    "ann_means = [0.947, 0.947, 0.925, 0.788, 0.948, 0.933]\n",
    "ann_stds = [0.018, 0.04, 0.07, 0.097, 0.036, 0.039]\n",
    "svm_means = [0.947, 0.947, 0.927, 0.953, 0.94, 0.4]\n",
    "svm_stds = [0.03, 0.038, 0.092, 0.051, 0.043, 0.082]\n",
    "dt_means = [0.927, 0.913, 0.913, 0.913, 0.913, 0.913]\n",
    "dt_stds = [0.043, 0.045, 0.045, 0.045, 0.045, 0.045]\n",
    "knn_means = [0.947, 0.947, 0.96, 0.94, 0.913, 0.507]\n",
    "knn_stds = [0.038, 0.038, 0.015, 0.043, 0.104, 0.068]\n",
    "\n",
    "# Create subplots for each model\n",
    "p1 = bar(1:6, ann_means, yerr=ann_stds, title=\"ANN\", legend=false)\n",
    "p2 = bar(1:6, svm_means, yerr=svm_stds, title=\"SVM\", legend=false)\n",
    "p3 = bar(1:6, dt_means, yerr=dt_stds, title=\"Decision Tree\", legend=false)\n",
    "p4 = bar(1:6, knn_means, yerr=knn_stds, title=\"KNN\", legend=false)\n",
    "\n",
    "# Customize the y-axis and labels\n",
    "for p in [p1, p2, p3, p4]\n",
    "    ylabel!(p, \"Accuracy\")\n",
    "    xlabel!(p, \"Set of Hyperparameters\")\n",
    "end\n",
    "\n",
    "# Combine the plots into one figure\n",
    "plot(p1, p2, p3, p4, layout=(2,2), size=(800,600))\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PyObject DecisionTreeClassifier(max_depth=10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"model = DecisionTreeClassifier(max_depth=10)\n",
    "\n",
    "# Train the model on the entire dataset\n",
    "fit!(model, train_inputs, train_targets)\n",
    "predictions = predict(model, test_inputs)\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"accuracy_value = accuracy(predictions, test_targets)\\nprintln(\\\"Accuracy: \\\", accuracy_value)\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GOTTA CHANGE THIS TO TAKE THE INPUTS AS THEY ARE RIGHT NOW\n",
    "\"\"\"\n",
    "accuracy_value = accuracy(predictions, test_targets)\n",
    "println(\"Accuracy: \", accuracy_value)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
