{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module DataLoader.\n",
      "WARNING: replacing module Visualization.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"    # ONE-HOT ENCODING\\n    function oneHotEncoding(feature::AbstractArray{<:Any,1},      \\n        classes::AbstractArray{<:Any,1})\\n    # First we are going to set a line as defensive to check values\\n    @assert(all([in(value, classes) for value in feature]));\\n    \\n    #\" ⋯ 9683 bytes ⋯ \"ights)\\n            WeightedAccuracy = sum(Accuracy .* Weights) / sum(Weights)\\n            return (Sensitivity, Specificity, PPV, NPV, F1, Accuracy, WeightedSensitivity, WeightedSpecificity, WeightedPPV, WeightedNPV, WeightedF1, WeightedAccuracy)\\n        end\\n    end\\nend\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"utils/data_loader.jl\")\n",
    "include(\"utils/visualization.jl\")\n",
    "include(\"utils/ml1_utils.jl\")\n",
    "include(\"utils/preprocessing.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "Random.seed!(123)\n",
    "\n",
    "data = DataLoader.load_data_for_analysis(\"dataset\\\\star_classification.csv\");\n",
    "#Visualization.entry_visualization(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_inputs before normalization: Float32[18.85256, 18.40885, 18.28408, 18.31415, 18.38598, 0.00333416]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I've changed the preprocess_data function so it doesn't OneHotEncode the targets\\n    because it's not needed&advised for KNN, DT & SVM, only for ANN. \\n    The OneHotEncoding for the ANN will be done in the modelCrossValidation function,\\n    which is called by the evaluateAndPrintMetricsRanking function.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DataFrames\n",
    "\n",
    "data = DataLoader.load_data(\"dataset\\\\star_classification.csv\");\n",
    "\n",
    "# Preprocess the data\n",
    "\n",
    "\"\"\"    This function does the following:\n",
    "        - Balance the data using the undersampling method if chosen to do so\n",
    "        - Parse the data: chosing the correct columns for inputs and targets (Shouldn't this be done before balancing??)\n",
    "        - Splits the data into training and testing using holdOut method\n",
    "        - Normalize the inputs  ------> zeromean method still to be implemented!!!!\n",
    "\"\"\"\n",
    "# preprocess_data(data, train_ratio, norm_method, balance, indices)\n",
    "train_inputs, train_targets, test_inputs, test_targets = Preprocessing.preprocess_data(data, 0.1, \"minmax\", true, [4,5,6,7,8,15]);\n",
    "\n",
    "\"\"\" I've changed the preprocess_data function so it doesn't OneHotEncode the targets\n",
    "    because it's not needed&advised for KNN, DT & SVM, only for ANN. \n",
    "    The OneHotEncoding for the ANN will be done in the modelCrossValidation function,\n",
    "    which is called by the evaluateAndPrintMetricsRanking function.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{SubString{String}, Float64} with 3 entries:\n",
       "  \"GALAXY\" => 0.334193\n",
       "  \"QSO\"    => 0.332474\n",
       "  \"STAR\"   => 0.333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using StatsBase\n",
    "\n",
    "counts = countmap(train_targets)\n",
    "total = length(train_targets)\n",
    "\n",
    "proportions = Dict(k => v / total for (k, v) in counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix{Float32}Vector{Any}Matrix{Float32}Vector{Any}\n",
      "(569, 6)(569,)(56314, 6)(56314,)\n",
      "Float32[0.29855812, 0.1377225, 0.3577274, 0.71487767, 0.42786127, 0.5657304, 0.6857209, 0.45806453, 0.7845978, 0.84025544]\n"
     ]
    }
   ],
   "source": [
    "println(typeof(train_inputs), typeof(train_targets),typeof(test_inputs), typeof(test_targets))\n",
    "println(size(train_inputs),size(train_targets),size(test_inputs),size(test_targets))\n",
    "println(train_inputs[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inputs: (569, 6)\n",
      "Train targets: (569,)\n",
      "Test inputs: (56314, 6)\n",
      "Test targets: (56314,)\n"
     ]
    }
   ],
   "source": [
    "# Check size of train and test sets\n",
    "println(\"Train inputs: \", size(train_inputs))\n",
    "println(\"Train targets: \", size(train_targets))\n",
    "println(\"Test inputs: \", size(test_inputs))\n",
    "println(\"Test targets: \", size(test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing hyperparameters for each model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.linear_model._logistic.LogisticRegression'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "\n",
    "@sk_import neural_network: MLPClassifier\n",
    "@sk_import svm: SVC\n",
    "@sk_import tree: DecisionTreeClassifier\n",
    "@sk_import neighbors: KNeighborsClassifier\n",
    "@sk_import ensemble: StackingClassifier  # For the stacking ensemble\n",
    "@sk_import linear_model: LogisticRegression  # For the final estimator in stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setting indices for the k-fold cross-validation\n",
    "    we are about to do with the different models\n",
    "\"\"\"\n",
    "N=size(train_inputs,1)\n",
    "k = 5 # number of folds\n",
    "kFoldIndices = crossvalidation(N, k);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with set of hyperparameters 1\n",
      "Training with set of hyperparameters 2\n",
      "Training with set of hyperparameters 3\n",
      "Training with set of hyperparameters 4\n",
      "Training with set of hyperparameters 5\n",
      "Training with set of hyperparameters 6\n",
      "\n",
      "----- acc -----\n",
      "Set of hyperparameters 2 -> mean: 0.947 Std. Dev.: 0.02\n",
      "Set of hyperparameters 3 -> mean: 0.944 Std. Dev.: 0.016\n",
      "Set of hyperparameters 4 -> mean: 0.944 Std. Dev.: 0.016\n",
      "Set of hyperparameters 5 -> mean: 0.944 Std. Dev.: 0.016\n",
      "Set of hyperparameters 6 -> mean: 0.944 Std. Dev.: 0.016\n",
      "Set of hyperparameters 1 -> mean: 0.921 Std. Dev.: 0.016\n",
      "\n",
      "----- sensitivity -----\n",
      "Set of hyperparameters 2 -> mean: 0.947 Std. Dev.: 0.02\n",
      "Set of hyperparameters 3 -> mean: 0.944 Std. Dev.: 0.016\n",
      "Set of hyperparameters 4 -> mean: 0.944 Std. Dev.: 0.016\n",
      "Set of hyperparameters 5 -> mean: 0.944 Std. Dev.: 0.016\n",
      "Set of hyperparameters 6 -> mean: 0.944 Std. Dev.: 0.016\n",
      "Set of hyperparameters 1 -> mean: 0.921 Std. Dev.: 0.016\n",
      "\n",
      "----- specificity -----\n",
      "Set of hyperparameters 2 -> mean: 0.976 Std. Dev.: 0.007\n",
      "Set of hyperparameters 3 -> mean: 0.974 Std. Dev.: 0.007\n",
      "Set of hyperparameters 4 -> mean: 0.974 Std. Dev.: 0.007\n",
      "Set of hyperparameters 5 -> mean: 0.974 Std. Dev.: 0.007\n",
      "Set of hyperparameters 6 -> mean: 0.974 Std. Dev.: 0.007\n",
      "Set of hyperparameters 1 -> mean: 0.965 Std. Dev.: 0.008\n",
      "\n",
      "----- ppv -----\n",
      "Set of hyperparameters 2 -> mean: 0.953 Std. Dev.: 0.014\n",
      "Set of hyperparameters 3 -> mean: 0.948 Std. Dev.: 0.012\n",
      "Set of hyperparameters 4 -> mean: 0.948 Std. Dev.: 0.012\n",
      "Set of hyperparameters 5 -> mean: 0.948 Std. Dev.: 0.012\n",
      "Set of hyperparameters 6 -> mean: 0.948 Std. Dev.: 0.012\n",
      "Set of hyperparameters 1 -> mean: 0.933 Std. Dev.: 0.013\n",
      "\n",
      "----- npv -----\n",
      "Set of hyperparameters 2 -> mean: 0.972 Std. Dev.: 0.015\n",
      "Set of hyperparameters 3 -> mean: 0.969 Std. Dev.: 0.014\n",
      "Set of hyperparameters 4 -> mean: 0.969 Std. Dev.: 0.014\n",
      "Set of hyperparameters 5 -> mean: 0.969 Std. Dev.: 0.014\n",
      "Set of hyperparameters 6 -> mean: 0.969 Std. Dev.: 0.014\n",
      "Set of hyperparameters 1 -> mean: 0.958 Std. Dev.: 0.015\n",
      "\n",
      "----- f_score -----\n",
      "Set of hyperparameters 2 -> mean: 0.948 Std. Dev.: 0.019\n",
      "Set of hyperparameters 3 -> mean: 0.944 Std. Dev.: 0.015\n",
      "Set of hyperparameters 4 -> mean: 0.944 Std. Dev.: 0.015\n",
      "Set of hyperparameters 5 -> mean: 0.944 Std. Dev.: 0.015\n",
      "Set of hyperparameters 6 -> mean: 0.944 Std. Dev.: 0.015\n",
      "Set of hyperparameters 1 -> mean: 0.921 Std. Dev.: 0.016\n",
      "\n",
      "----- err_rate -----\n",
      "Set of hyperparameters 2 -> mean: 0.053 Std. Dev.: 0.02\n",
      "Set of hyperparameters 3 -> mean: 0.056 Std. Dev.: 0.016\n",
      "Set of hyperparameters 4 -> mean: 0.056 Std. Dev.: 0.016\n",
      "Set of hyperparameters 5 -> mean: 0.056 Std. Dev.: 0.016\n",
      "Set of hyperparameters 6 -> mean: 0.056 Std. Dev.: 0.016\n",
      "Set of hyperparameters 1 -> mean: 0.079 Std. Dev.: 0.016\n"
     ]
    }
   ],
   "source": [
    "# Define an array of hyperparameter dictionaries for the Decision Tree model\n",
    "dtree_hyperparameters_array = [\n",
    "    Dict(\"max_depth\" => 3),\n",
    "    Dict(\"max_depth\" => 5),\n",
    "    Dict(\"max_depth\" => 10),\n",
    "    Dict(\"max_depth\" => 20),\n",
    "    Dict(\"max_depth\" => 50),\n",
    "    Dict(\"max_depth\" => 100) # Deeper trees can capture more detail but risk overfitting\n",
    "]\n",
    "\n",
    "# Call the function to evaluate the model using different sets of hyperparameters and print the ranking of metrics.\n",
    "evaluateAndPrintMetricsRanking(:DecisionTree,dtree_hyperparameters_array, train_inputs, train_targets, kFoldIndices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with set of hyperparameters 1\n",
      "Training with set of hyperparameters 2\n",
      "Training with set of hyperparameters 3\n",
      "Training with set of hyperparameters 4\n",
      "Training with set of hyperparameters 5\n",
      "Training with set of hyperparameters 6\n",
      "\n",
      "----- acc -----\n",
      "Set of hyperparameters 1 -> mean: 0.881 Std. Dev.: 0.024\n",
      "Set of hyperparameters 3 -> mean: 0.845 Std. Dev.: 0.03\n",
      "Set of hyperparameters 2 -> mean: 0.842 Std. Dev.: 0.036\n",
      "Set of hyperparameters 4 -> mean: 0.826 Std. Dev.: 0.042\n",
      "Set of hyperparameters 5 -> mean: 0.787 Std. Dev.: 0.041\n",
      "Set of hyperparameters 6 -> mean: 0.75 Std. Dev.: 0.03\n",
      "\n",
      "----- sensitivity -----\n",
      "Set of hyperparameters 1 -> mean: 0.881 Std. Dev.: 0.024\n",
      "Set of hyperparameters 3 -> mean: 0.845 Std. Dev.: 0.03\n",
      "Set of hyperparameters 2 -> mean: 0.842 Std. Dev.: 0.036\n",
      "Set of hyperparameters 4 -> mean: 0.826 Std. Dev.: 0.042\n",
      "Set of hyperparameters 5 -> mean: 0.787 Std. Dev.: 0.041\n",
      "Set of hyperparameters 6 -> mean: 0.75 Std. Dev.: 0.03\n",
      "\n",
      "----- specificity -----\n",
      "Set of hyperparameters 1 -> mean: 0.942 Std. Dev.: 0.013\n",
      "Set of hyperparameters 3 -> mean: 0.926 Std. Dev.: 0.018\n",
      "Set of hyperparameters 2 -> mean: 0.924 Std. Dev.: 0.022\n",
      "Set of hyperparameters 4 -> mean: 0.915 Std. Dev.: 0.028\n",
      "Set of hyperparameters 5 -> mean: 0.896 Std. Dev.: 0.027\n",
      "Set of hyperparameters 6 -> mean: 0.876 Std. Dev.: 0.02\n",
      "\n",
      "----- ppv -----\n",
      "Set of hyperparameters 1 -> mean: 0.887 Std. Dev.: 0.023\n",
      "Set of hyperparameters 3 -> mean: 0.86 Std. Dev.: 0.033\n",
      "Set of hyperparameters 2 -> mean: 0.859 Std. Dev.: 0.039\n",
      "Set of hyperparameters 4 -> mean: 0.845 Std. Dev.: 0.043\n",
      "Set of hyperparameters 5 -> mean: 0.805 Std. Dev.: 0.042\n",
      "Set of hyperparameters 6 -> mean: 0.761 Std. Dev.: 0.037\n",
      "\n",
      "----- npv -----\n",
      "Set of hyperparameters 1 -> mean: 0.936 Std. Dev.: 0.015\n",
      "Set of hyperparameters 3 -> mean: 0.919 Std. Dev.: 0.019\n",
      "Set of hyperparameters 2 -> mean: 0.918 Std. Dev.: 0.022\n",
      "Set of hyperparameters 4 -> mean: 0.91 Std. Dev.: 0.026\n",
      "Set of hyperparameters 5 -> mean: 0.89 Std. Dev.: 0.026\n",
      "Set of hyperparameters 6 -> mean: 0.872 Std. Dev.: 0.017\n",
      "\n",
      "----- f_score -----\n",
      "Set of hyperparameters 1 -> mean: 0.882 Std. Dev.: 0.024\n",
      "Set of hyperparameters 3 -> mean: 0.847 Std. Dev.: 0.03\n",
      "Set of hyperparameters 2 -> mean: 0.843 Std. Dev.: 0.035\n",
      "Set of hyperparameters 4 -> mean: 0.827 Std. Dev.: 0.042\n",
      "Set of hyperparameters 5 -> mean: 0.789 Std. Dev.: 0.039\n",
      "Set of hyperparameters 6 -> mean: 0.752 Std. Dev.: 0.032\n",
      "\n",
      "----- err_rate -----\n",
      "Set of hyperparameters 1 -> mean: 0.119 Std. Dev.: 0.024\n",
      "Set of hyperparameters 3 -> mean: 0.155 Std. Dev.: 0.03\n",
      "Set of hyperparameters 2 -> mean: 0.158 Std. Dev.: 0.036\n",
      "Set of hyperparameters 4 -> mean: 0.174 Std. Dev.: 0.042\n",
      "Set of hyperparameters 5 -> mean: 0.213 Std. Dev.: 0.041\n",
      "Set of hyperparameters 6 -> mean: 0.25 Std. Dev.: 0.03\n"
     ]
    }
   ],
   "source": [
    "# Define an array of hyperparameter dictionaries for the kNN model\n",
    "knn_hyperparameters_array = [\n",
    "    Dict(\"n_neighbors\" => 5),\n",
    "    Dict(\"n_neighbors\" => 10),\n",
    "    Dict(\"n_neighbors\" => 15),\n",
    "    Dict(\"n_neighbors\" => 20),\n",
    "    Dict(\"n_neighbors\" => 50),\n",
    "    Dict(\"n_neighbors\" => 100) # Large neighborhoods, smooths out predictions\n",
    "]\n",
    "\n",
    "# Call the function to evaluate the model using different sets of hyperparameters and print the ranking of metrics.\n",
    "evaluateAndPrintMetricsRanking(:kNN,knn_hyperparameters_array, train_inputs, train_targets, kFoldIndices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with set of hyperparameters 1\n",
      "Training with set of hyperparameters 2\n",
      "Training with set of hyperparameters 3\n",
      "Training with set of hyperparameters 4\n",
      "Training with set of hyperparameters 5\n",
      "Training with set of hyperparameters 6\n",
      "Training with set of hyperparameters 7\n",
      "Training with set of hyperparameters 8\n",
      "\n",
      "----- acc -----\n",
      "Set of hyperparameters 5 -> mean: 0.93 Std. Dev.: 0.014\n",
      "Set of hyperparameters 7 -> mean: 0.93 Std. Dev.: 0.014\n",
      "Set of hyperparameters 1 -> mean: 0.93 Std. Dev.: 0.015\n",
      "Set of hyperparameters 3 -> mean: 0.929 Std. Dev.: 0.015\n",
      "Set of hyperparameters 8 -> mean: 0.908 Std. Dev.: 0.024\n",
      "Set of hyperparameters 2 -> mean: 0.905 Std. Dev.: 0.019\n",
      "Set of hyperparameters 6 -> mean: 0.829 Std. Dev.: 0.041\n",
      "Set of hyperparameters 4 -> mean: 0.394 Std. Dev.: 0.047\n",
      "\n",
      "----- sensitivity -----\n",
      "Set of hyperparameters 5 -> mean: 0.93 Std. Dev.: 0.014\n",
      "Set of hyperparameters 7 -> mean: 0.93 Std. Dev.: 0.014\n",
      "Set of hyperparameters 1 -> mean: 0.93 Std. Dev.: 0.015\n",
      "Set of hyperparameters 3 -> mean: 0.929 Std. Dev.: 0.015\n",
      "Set of hyperparameters 8 -> mean: 0.908 Std. Dev.: 0.024\n",
      "Set of hyperparameters 2 -> mean: 0.905 Std. Dev.: 0.019\n",
      "Set of hyperparameters 6 -> mean: 0.829 Std. Dev.: 0.041\n",
      "Set of hyperparameters 4 -> mean: 0.394 Std. Dev.: 0.047\n",
      "\n",
      "----- specificity -----\n",
      "Set of hyperparameters 7 -> mean: 0.968 Std. Dev.: 0.007\n",
      "Set of hyperparameters 3 -> mean: 0.967 Std. Dev.: 0.007\n",
      "Set of hyperparameters 1 -> mean: 0.967 Std. Dev.: 0.006\n",
      "Set of hyperparameters 5 -> mean: 0.967 Std. Dev.: 0.007\n",
      "Set of hyperparameters 8 -> mean: 0.956 Std. Dev.: 0.012\n",
      "Set of hyperparameters 2 -> mean: 0.955 Std. Dev.: 0.01\n",
      "Set of hyperparameters 6 -> mean: 0.918 Std. Dev.: 0.017\n",
      "Set of hyperparameters 4 -> mean: 0.7 Std. Dev.: 0.027\n",
      "\n",
      "----- ppv -----\n",
      "Set of hyperparameters 7 -> mean: 0.937 Std. Dev.: 0.01\n",
      "Set of hyperparameters 1 -> mean: 0.936 Std. Dev.: 0.01\n",
      "Set of hyperparameters 5 -> mean: 0.936 Std. Dev.: 0.011\n",
      "Set of hyperparameters 3 -> mean: 0.936 Std. Dev.: 0.01\n",
      "Set of hyperparameters 8 -> mean: 0.915 Std. Dev.: 0.022\n",
      "Set of hyperparameters 2 -> mean: 0.913 Std. Dev.: 0.017\n",
      "Set of hyperparameters 6 -> mean: 0.852 Std. Dev.: 0.029\n",
      "Set of hyperparameters 4 -> mean: 0.741 Std. Dev.: 0.011\n",
      "\n",
      "----- npv -----\n",
      "Set of hyperparameters 5 -> mean: 0.964 Std. Dev.: 0.008\n",
      "Set of hyperparameters 7 -> mean: 0.963 Std. Dev.: 0.009\n",
      "Set of hyperparameters 3 -> mean: 0.963 Std. Dev.: 0.008\n",
      "Set of hyperparameters 1 -> mean: 0.963 Std. Dev.: 0.01\n",
      "Set of hyperparameters 8 -> mean: 0.952 Std. Dev.: 0.016\n",
      "Set of hyperparameters 2 -> mean: 0.951 Std. Dev.: 0.013\n",
      "Set of hyperparameters 6 -> mean: 0.918 Std. Dev.: 0.023\n",
      "Set of hyperparameters 4 -> mean: 0.77 Std. Dev.: 0.013\n",
      "\n",
      "----- f_score -----\n",
      "Set of hyperparameters 7 -> mean: 0.93 Std. Dev.: 0.013\n",
      "Set of hyperparameters 5 -> mean: 0.93 Std. Dev.: 0.013\n",
      "Set of hyperparameters 1 -> mean: 0.93 Std. Dev.: 0.014\n",
      "Set of hyperparameters 3 -> mean: 0.929 Std. Dev.: 0.015\n",
      "Set of hyperparameters 8 -> mean: 0.907 Std. Dev.: 0.023\n",
      "Set of hyperparameters 2 -> mean: 0.904 Std. Dev.: 0.018\n",
      "Set of hyperparameters 6 -> mean: 0.819 Std. Dev.: 0.058\n",
      "Set of hyperparameters 4 -> mean: 0.267 Std. Dev.: 0.066\n",
      "\n",
      "----- err_rate -----\n",
      "Set of hyperparameters 5 -> mean: 0.07 Std. Dev.: 0.014\n",
      "Set of hyperparameters 7 -> mean: 0.07 Std. Dev.: 0.014\n",
      "Set of hyperparameters 1 -> mean: 0.07 Std. Dev.: 0.015\n",
      "Set of hyperparameters 3 -> mean: 0.071 Std. Dev.: 0.015\n",
      "Set of hyperparameters 8 -> mean: 0.092 Std. Dev.: 0.024\n",
      "Set of hyperparameters 2 -> mean: 0.095 Std. Dev.: 0.019\n",
      "Set of hyperparameters 6 -> mean: 0.171 Std. Dev.: 0.041\n",
      "Set of hyperparameters 4 -> mean: 0.606 Std. Dev.: 0.047\n"
     ]
    }
   ],
   "source": [
    "# Define an array of hyperparameter dictionaries for the ANN model\n",
    "ann_hyperparameters_array = [\n",
    "    # Two-layer architecture, moderate neurons\n",
    "    Dict(\"architecture\" => [50, 30], \"activation\" => \"relu\", \"learning_rate\" => 0.01, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # One-layer architecture, fewer neurons\n",
    "    Dict(\"architecture\" => [30], \"activation\" => \"relu\", \"learning_rate\" => 0.01, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # Two-layer, different activation function\n",
    "    Dict(\"architecture\" => [50, 30], \"activation\" => \"tanh\", \"learning_rate\" => 0.01, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # One-layer, lower learning rate\n",
    "    Dict(\"architecture\" => [30], \"activation\" => \"relu\", \"learning_rate\" => 0.001, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 2000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # Two-layer, higher learning rate\n",
    "    Dict(\"architecture\" => [50, 30], \"activation\" => \"relu\", \"learning_rate\" => 0.05, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # One-layer, logistic activation\n",
    "    Dict(\"architecture\" => [30], \"activation\" => \"logistic\", \"learning_rate\" => 0.01, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # Two-layer, more neurons, different activation\n",
    "    Dict(\"architecture\" => [70, 40], \"activation\" => \"tanh\", \"learning_rate\" => 0.01, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10),\n",
    "\n",
    "    # One-layer, more neurons\n",
    "    Dict(\"architecture\" => [50], \"activation\" => \"relu\", \"learning_rate\" => 0.01, \"validation_ratio\" => 0.1, \"n_iter_no_change\" => 80, \"max_iter\" => 1000, \"repetitionsTraining\" => 10)\n",
    "]\n",
    "\n",
    "# Call the function to evaluate the model using different sets of hyperparameters and print the ranking of metrics.\n",
    "evaluateAndPrintMetricsRanking(:ANN, ann_hyperparameters_array, train_inputs, train_targets, kFoldIndices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with set of hyperparameters 1\n",
      "Training with set of hyperparameters 2\n",
      "Training with set of hyperparameters 3\n",
      "Training with set of hyperparameters 4\n",
      "Training with set of hyperparameters 5\n",
      "Training with set of hyperparameters 6\n",
      "Training with set of hyperparameters 7\n",
      "Training with set of hyperparameters 8\n",
      "Training with set of hyperparameters 9\n",
      "\n",
      "----- acc -----\n",
      "Set of hyperparameters 8 -> mean: 0.9 Std. Dev.: 0.029\n",
      "Set of hyperparameters 1 -> mean: 0.879 Std. Dev.: 0.024\n",
      "Set of hyperparameters 7 -> mean: 0.87 Std. Dev.: 0.016\n",
      "Set of hyperparameters 2 -> mean: 0.861 Std. Dev.: 0.027\n",
      "Set of hyperparameters 5 -> mean: 0.793 Std. Dev.: 0.033\n",
      "Set of hyperparameters 6 -> mean: 0.793 Std. Dev.: 0.033\n",
      "Set of hyperparameters 3 -> mean: 0.772 Std. Dev.: 0.022\n",
      "Set of hyperparameters 4 -> mean: 0.663 Std. Dev.: 0.051\n",
      "Set of hyperparameters 9 -> mean: 0.353 Std. Dev.: 0.066\n",
      "\n",
      "----- sensitivity -----\n",
      "Set of hyperparameters 8 -> mean: 0.9 Std. Dev.: 0.029\n",
      "Set of hyperparameters 1 -> mean: 0.879 Std. Dev.: 0.024\n",
      "Set of hyperparameters 7 -> mean: 0.87 Std. Dev.: 0.016\n",
      "Set of hyperparameters 2 -> mean: 0.861 Std. Dev.: 0.027\n",
      "Set of hyperparameters 5 -> mean: 0.793 Std. Dev.: 0.033\n",
      "Set of hyperparameters 6 -> mean: 0.793 Std. Dev.: 0.033\n",
      "Set of hyperparameters 3 -> mean: 0.772 Std. Dev.: 0.022\n",
      "Set of hyperparameters 4 -> mean: 0.663 Std. Dev.: 0.051\n",
      "Set of hyperparameters 9 -> mean: 0.353 Std. Dev.: 0.066\n",
      "\n",
      "----- specificity -----\n",
      "Set of hyperparameters 8 -> mean: 0.951 Std. Dev.: 0.013\n",
      "Set of hyperparameters 1 -> mean: 0.942 Std. Dev.: 0.009\n",
      "Set of hyperparameters 7 -> mean: 0.936 Std. Dev.: 0.009\n",
      "Set of hyperparameters 2 -> mean: 0.932 Std. Dev.: 0.019\n",
      "Set of hyperparameters 5 -> mean: 0.897 Std. Dev.: 0.02\n",
      "Set of hyperparameters 6 -> mean: 0.897 Std. Dev.: 0.02\n",
      "Set of hyperparameters 3 -> mean: 0.886 Std. Dev.: 0.019\n",
      "Set of hyperparameters 4 -> mean: 0.844 Std. Dev.: 0.022\n",
      "Set of hyperparameters 9 -> mean: 0.727 Std. Dev.: 0.016\n",
      "\n",
      "----- ppv -----\n",
      "Set of hyperparameters 8 -> mean: 0.906 Std. Dev.: 0.03\n",
      "Set of hyperparameters 1 -> mean: 0.893 Std. Dev.: 0.021\n",
      "Set of hyperparameters 7 -> mean: 0.889 Std. Dev.: 0.016\n",
      "Set of hyperparameters 2 -> mean: 0.878 Std. Dev.: 0.031\n",
      "Set of hyperparameters 5 -> mean: 0.801 Std. Dev.: 0.035\n",
      "Set of hyperparameters 6 -> mean: 0.801 Std. Dev.: 0.035\n",
      "Set of hyperparameters 3 -> mean: 0.787 Std. Dev.: 0.023\n",
      "Set of hyperparameters 4 -> mean: 0.718 Std. Dev.: 0.042\n",
      "Set of hyperparameters 9 -> mean: 0.682 Std. Dev.: 0.099\n",
      "\n",
      "----- npv -----\n",
      "Set of hyperparameters 8 -> mean: 0.951 Std. Dev.: 0.02\n",
      "Set of hyperparameters 7 -> mean: 0.94 Std. Dev.: 0.017\n",
      "Set of hyperparameters 1 -> mean: 0.938 Std. Dev.: 0.018\n",
      "Set of hyperparameters 2 -> mean: 0.932 Std. Dev.: 0.017\n",
      "Set of hyperparameters 5 -> mean: 0.893 Std. Dev.: 0.02\n",
      "Set of hyperparameters 6 -> mean: 0.893 Std. Dev.: 0.02\n",
      "Set of hyperparameters 3 -> mean: 0.882 Std. Dev.: 0.012\n",
      "Set of hyperparameters 4 -> mean: 0.836 Std. Dev.: 0.02\n",
      "Set of hyperparameters 9 -> mean: 0.74 Std. Dev.: 0.034\n",
      "\n",
      "----- f_score -----\n",
      "Set of hyperparameters 8 -> mean: 0.897 Std. Dev.: 0.031\n",
      "Set of hyperparameters 1 -> mean: 0.878 Std. Dev.: 0.024\n",
      "Set of hyperparameters 7 -> mean: 0.866 Std. Dev.: 0.017\n",
      "Set of hyperparameters 2 -> mean: 0.859 Std. Dev.: 0.028\n",
      "Set of hyperparameters 5 -> mean: 0.793 Std. Dev.: 0.033\n",
      "Set of hyperparameters 6 -> mean: 0.793 Std. Dev.: 0.033\n",
      "Set of hyperparameters 3 -> mean: 0.775 Std. Dev.: 0.021\n",
      "Set of hyperparameters 4 -> mean: 0.641 Std. Dev.: 0.108\n",
      "Set of hyperparameters 9 -> mean: 0.231 Std. Dev.: 0.091\n",
      "\n",
      "----- err_rate -----\n",
      "Set of hyperparameters 8 -> mean: 0.1 Std. Dev.: 0.029\n",
      "Set of hyperparameters 1 -> mean: 0.121 Std. Dev.: 0.024\n",
      "Set of hyperparameters 7 -> mean: 0.13 Std. Dev.: 0.016\n",
      "Set of hyperparameters 2 -> mean: 0.139 Std. Dev.: 0.027\n",
      "Set of hyperparameters 5 -> mean: 0.207 Std. Dev.: 0.033\n",
      "Set of hyperparameters 6 -> mean: 0.207 Std. Dev.: 0.033\n",
      "Set of hyperparameters 3 -> mean: 0.228 Std. Dev.: 0.022\n",
      "Set of hyperparameters 4 -> mean: 0.337 Std. Dev.: 0.051\n",
      "Set of hyperparameters 9 -> mean: 0.647 Std. Dev.: 0.066\n"
     ]
    }
   ],
   "source": [
    "svm_hyperparameters_array = [\n",
    "    # Uses 'rbf' kernel, medium complexity with C=1.0, default polynomial degree, 'scale' for gamma \n",
    "    Dict(\"kernel\" => \"rbf\", \"degree\" => 3, \"C\" => 1.0, \"gamma\" => \"scale\"),\n",
    "    \n",
    "    # Same 'rbf' kernel, increased penalty (C=10.0) for larger-margin separation, 'auto' gamma adjusts based on features\n",
    "    Dict(\"kernel\" => \"rbf\", \"degree\" => 3, \"C\" => 10.0, \"gamma\" => \"auto\"),\n",
    "    \n",
    "    # Same 'rbf' kernel, lower penalty (C=0.1) for a softer-margin, 'scale' gamma is default scaling\n",
    "    Dict(\"kernel\" => \"rbf\", \"degree\" => 3, \"C\" => 0.1, \"gamma\" => \"scale\"),\n",
    "\n",
    "    # 'linear' kernel, suitable for less complex data\n",
    "    Dict(\"kernel\" => \"linear\", \"degree\" => 3, \"C\" => 0.1, \"gamma\" => \"auto\"),\n",
    "    \n",
    "    # 'linear' kernel, with C=1.0 indicating a balance between margin and misclassification\n",
    "    Dict(\"kernel\" => \"linear\", \"degree\" => 3, \"C\" => 1.0, \"gamma\" => \"auto\"),\n",
    "\n",
    "    # 'linear' kernel, with a medium penalty and scale gamma\n",
    "    Dict(\"kernel\" => \"linear\", \"degree\" => 3, \"C\" => 1.0, \"gamma\" => \"scale\"),\n",
    "\n",
    "    # 'linear' kernel with a higher penalty and scale gamma\n",
    "    Dict(\"kernel\" => \"linear\", \"degree\" => 3, \"C\" => 10.0, \"gamma\" => \"scale\"),\n",
    "    \n",
    "    # 'poly' kernel, polynomial degree is set twice by mistake, should only be 'degree' => 3, 'scale' gamma defaults to feature scale\n",
    "    Dict(\"kernel\" => \"poly\", \"degree\" => 3, \"C\" => 1.0, \"gamma\" => \"scale\"),\n",
    "    \n",
    "    # 'poly' kernel, increased polynomial degree (5) for higher model complexity, 'auto' gamma may overfit with high dimension\n",
    "    Dict(\"kernel\" => \"poly\", \"degree\" => 5, \"C\" => 1.0, \"gamma\" => \"auto\")\n",
    "]\n",
    "\n",
    "\n",
    "# Call the function to evaluate the model using different sets of hyperparameters and print the ranking of metrics.\n",
    "evaluateAndPrintMetricsRanking(:SVM, svm_hyperparameters_array, train_inputs, train_targets, kFoldIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "bar recipe: x must be same length as y (centers), or one more than y (edges).\n\t\tlength(x)=6, length(y)=8",
     "output_type": "error",
     "traceback": [
      "bar recipe: x must be same length as y (centers), or one more than y (edges).\n\t\tlength(x)=6, length(y)=8",
      "",
      "Stacktrace:",
      "  [1] error(s::String)",
      "    @ Base .\\error.jl:35",
      "  [2] macro expansion",
      "    @ C:\\Users\\marci\\.julia\\packages\\Plots\\sxUvK\\src\\recipes.jl:416 [inlined]",
      "  [3] apply_recipe(plotattributes::AbstractDict{Symbol, Any}, #unused#::Type{Val{:bar}}, x::Any, y::Any, z::Any)",
      "    @ Plots C:\\Users\\marci\\.julia\\packages\\RecipesBase\\BRe07\\src\\RecipesBase.jl:300",
      "  [4] _process_seriesrecipe(plt::Any, plotattributes::Any)",
      "    @ RecipesPipeline C:\\Users\\marci\\.julia\\packages\\RecipesPipeline\\BGM3l\\src\\series_recipe.jl:50",
      "  [5] _process_seriesrecipes!(plt::Any, kw_list::Any)",
      "    @ RecipesPipeline C:\\Users\\marci\\.julia\\packages\\RecipesPipeline\\BGM3l\\src\\series_recipe.jl:27",
      "  [6] recipe_pipeline!(plt::Any, plotattributes::Any, args::Any)",
      "    @ RecipesPipeline C:\\Users\\marci\\.julia\\packages\\RecipesPipeline\\BGM3l\\src\\RecipesPipeline.jl:99",
      "  [7] _plot!(plt::Plots.Plot, plotattributes::Any, args::Any)",
      "    @ Plots C:\\Users\\marci\\.julia\\packages\\Plots\\sxUvK\\src\\plot.jl:223",
      "  [8] plot(::Any, ::Vararg{Any}; kw::Base.Pairs{Symbol, V, Tuple{Vararg{Symbol, N}}, NamedTuple{names, T}} where {V, N, names, T<:Tuple{Vararg{Any, N}}})",
      "    @ Plots C:\\Users\\marci\\.julia\\packages\\Plots\\sxUvK\\src\\plot.jl:102",
      "  [9] bar(::Any, ::Vararg{Any}; kw::Base.Pairs{Symbol, V, Tuple{Vararg{Symbol, N}}, NamedTuple{names, T}} where {V, N, names, T<:Tuple{Vararg{Any, N}}})",
      "    @ Plots C:\\Users\\marci\\.julia\\packages\\RecipesBase\\BRe07\\src\\RecipesBase.jl:427",
      " [10] top-level scope",
      "    @ In[1]:14"
     ]
    }
   ],
   "source": [
    "\"\n",
    "using Plots\n",
    "\n",
    "# Define the data for each model\n",
    "ann_means = [0.947, 0.947, 0.925, 0.788, 0.948, 0.933, 0.8, 0.9]\n",
    "ann_stds = [0.018, 0.04, 0.07, 0.097, 0.036, 0.039, 0.4, 0.3]\n",
    "svm_means = [0.947, 0.947, 0.927, 0.953, 0.94, 0.4, 0.5]\n",
    "svm_stds = [0.03, 0.038, 0.092, 0.051, 0.043, 0.082, 0.7]\n",
    "dt_means = [0.927, 0.913, 0.913, 0.913, 0.913, 0.913]\n",
    "dt_stds = [0.043, 0.045, 0.045, 0.045, 0.045, 0.045]\n",
    "knn_means = [0.947, 0.947, 0.96, 0.94, 0.913, 0.507]\n",
    "knn_stds = [0.038, 0.038, 0.015, 0.043, 0.104, 0.068]\n",
    "\n",
    "# Create subplots for each model\n",
    "p1 = bar(1:6, ann_means, yerr=ann_stds, title=\"ANN\", legend=false)\n",
    "p2 = bar(1:6, svm_means, yerr=svm_stds, title=\"SVM\", legend=false)\n",
    "p3 = bar(1:6, dt_means, yerr=dt_stds, title=\"Decision Tree\", legend=false)\n",
    "p4 = bar(1:6, knn_means, yerr=knn_stds, title=\"KNN\", legend=false)\n",
    "\n",
    "# Customize the y-axis and labels\n",
    "for p in [p1, p2, p3, p4]\n",
    "    ylabel!(p, \"Accuracy\")\n",
    "    xlabel!(p, \"Set of Hyperparameters\")\n",
    "end\n",
    "\n",
    "# Combine the plots into one figure\n",
    "plot(p1, p2, p3, p4, layout=(2,2), size=(800,600))\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree model accuracy: 85.74777142451255 %\n"
     ]
    }
   ],
   "source": [
    "using JLD\n",
    "\n",
    "dt_model = DecisionTreeClassifier(max_depth=10)\n",
    "\n",
    "# Fit the model on the training data\n",
    "fit!(dt_model, train_inputs, train_targets)\n",
    "\n",
    "# Predict the targets for the test data\n",
    "predicted_targets = predict(dt_model, test_inputs)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "dt_model_accuracy = mean(predicted_targets .== test_targets)\n",
    "println(\"Decision Tree model accuracy: $(dt_model_accuracy * 100) %\")\n",
    "\n",
    "# Save the model\n",
    "#JLD.save(\"dt_model.jld\", \"model\", dt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN model accuracy: 36.65695919309586 %\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# Fit the model on the training data\n",
    "fit!(knn_model, train_inputs, train_targets)\n",
    "\n",
    "# Predict the targets for the test data\n",
    "predicted_targets = predict(knn_model, test_inputs)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "knn_model_accuracy = mean(predicted_targets .== test_targets)\n",
    "println(\"KNN model accuracy: $(knn_model_accuracy * 100) %\")\n",
    "\n",
    "# Save the model\n",
    "#JLD.save(\"knn_model.jld\", \"model\", knn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN model accuracy: 74.74340306140569 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\coros\\.julia\\conda\\3\\x86_64\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ann_model = MLPClassifier(hidden_layer_sizes=(70, 40), activation=\"tanh\", learning_rate_init=0.01, validation_fraction=0.1, n_iter_no_change=80, max_iter=1000)\n",
    "\n",
    "# Fit the model on the training data\n",
    "fit!(ann_model, train_inputs, train_targets)\n",
    "\n",
    "# Predict the targets for the test data\n",
    "predicted_targets = predict(ann_model, test_inputs)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "ann_model_accuracy = mean(predicted_targets .== test_targets)\n",
    "println(\"ANN model accuracy: $(ann_model_accuracy * 100) %\")\n",
    "\n",
    "# Save the model\n",
    "#JLD.save(\"ann_model.jld\", \"model\", ann_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model accuracy: 49.40334552686721 %\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(kernel=\"rbf\", degree=3, C=1.0, gamma=\"scale\")\n",
    "\n",
    "# Fit the model on the training data\n",
    "fit!(svm_model, train_inputs, train_targets)\n",
    "\n",
    "# Predict the targets for the test data\n",
    "predicted_targets = predict(svm_model, test_inputs)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "knn_model_accuracy = mean(predicted_targets .== test_targets)\n",
    "println(\"SVM model accuracy: $(knn_model_accuracy * 100) %\")\n",
    "\n",
    "# Save the model\n",
    "#JLD.save(\"svm_model.jld\", \"model\", svm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble model accuracy: 60.430798735660765 %\n"
     ]
    }
   ],
   "source": [
    "# Define the base models with the chosen hyperparameters\n",
    "dt_model = DecisionTreeClassifier(max_depth=10)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=10)\n",
    "ann_model = MLPClassifier(hidden_layer_sizes=(70, 40), activation=\"tanh\", learning_rate_init=0.01, validation_fraction=0.1, n_iter_no_change=80, max_iter=10000) # Increase max_iter from 1000 to ensure convergence\n",
    "svm_model = SVC(kernel=\"rbf\", degree=3, C=1.0, gamma=\"scale\")\n",
    "\n",
    "# Create a list of tuples (name, model) for the base models\n",
    "base_models = [\n",
    "    (\"DecisionTree\", dt_model),\n",
    "    (\"kNN\", knn_model),\n",
    "    (\"ANN\", ann_model),\n",
    "    (\"SVM\", svm_model)\n",
    "]\n",
    "\n",
    "# Choose a final estimator for the stacking ensemble\n",
    "# Logistic Regression is a common choice for combining predictions\n",
    "final_estimator = LogisticRegression()\n",
    "\n",
    "# Create the stacking ensemble\n",
    "ensemble = StackingClassifier(estimators=base_models, final_estimator=final_estimator)\n",
    "\n",
    "# Train the ensemble model\n",
    "fit!(ensemble, train_inputs, train_targets)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "model_accuracy = score(ensemble, test_inputs, test_targets)\n",
    "println(\"Ensemble model accuracy: $(model_accuracy * 100) %\")\n",
    "\n",
    "# Save the model\n",
    "#JLD.save(\"ensemble.jld\", \"model\", ensemble)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"using Plots\\n\\n# Define the data for each model\\nann_means = [0.947, 0.947, 0.925, 0.788, 0.948, 0.933, 0.8, 0.9]\\nann_stds = [0.018, 0.04, 0.07, 0.097, 0.036, 0.039, 0.4, 0.3]\\nsvm_means = [0.947, 0.947, 0.927, 0.953, 0.94, 0.4, 0.5]\\nsvm_stds = [0.03, 0.038, 0.092, 0.051, 0.\" ⋯ 492 bytes ⋯ \"means, yerr=knn_stds, title=\\\"KNN\\\", legend=false)\\n\\n# Customize the y-axis and labels\\nfor p in [p1, p2, p3, p4]\\n    ylabel!(p, \\\"Accuracy\\\")\\n    xlabel!(p, \\\"Set of Hyperparameters\\\")\\nend\\n\\n# Combine the plots into one figure\\nplot(p1, p2, p3, p4, layout=(2,2), size=(800,600))\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "using Plots\n",
    "\n",
    "# Define the data for each model\n",
    "ann_means = [0.947, 0.947, 0.925, 0.788, 0.948, 0.933, 0.8, 0.9]\n",
    "ann_stds = [0.018, 0.04, 0.07, 0.097, 0.036, 0.039, 0.4, 0.3]\n",
    "svm_means = [0.947, 0.947, 0.927, 0.953, 0.94, 0.4, 0.5]\n",
    "svm_stds = [0.03, 0.038, 0.092, 0.051, 0.043, 0.082, 0.7]\n",
    "dt_means = [0.927, 0.913, 0.913, 0.913, 0.913, 0.913]\n",
    "dt_stds = [0.043, 0.045, 0.045, 0.045, 0.045, 0.045]\n",
    "knn_means = [0.947, 0.947, 0.96, 0.94, 0.913, 0.507]\n",
    "knn_stds = [0.038, 0.038, 0.015, 0.043, 0.104, 0.068]\n",
    "\n",
    "# Create subplots for each model\n",
    "p1 = bar(1:6, ann_means, yerr=ann_stds, title=\"ANN\", legend=false)\n",
    "p2 = bar(1:6, svm_means, yerr=svm_stds, title=\"SVM\", legend=false)\n",
    "p3 = bar(1:6, dt_means, yerr=dt_stds, title=\"Decision Tree\", legend=false)\n",
    "p4 = bar(1:6, knn_means, yerr=knn_stds, title=\"KNN\", legend=false)\n",
    "\n",
    "# Customize the y-axis and labels\n",
    "for p in [p1, p2, p3, p4]\n",
    "    ylabel!(p, \"Accuracy\")\n",
    "    xlabel!(p, \"Set of Hyperparameters\")\n",
    "end\n",
    "\n",
    "# Combine the plots into one figure\n",
    "plot(p1, p2, p3, p4, layout=(2,2), size=(800,600))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy-pasted from unit 6 but something similar would go here ##\n",
    "\n",
    "### Best model configuration\n",
    "\n",
    "Based on the hyperparameters provided for each model and the results we have, the best configurations for each model according to **accuracy** are:\n",
    "\n",
    "\n",
    "- **Artificial Neural Network (ANN)**: The best-performing ANN model uses the architecture [100, 100, 100] with 'relu' activation, a learning rate of 0.01, a validation ratio of 0.1, and a maximum of 1000 iterations. This suggests that a more complex model with a higher number of neurons was able to capture the complexity of the data better than simpler models.\n",
    "\n",
    "- **Support Vector Machine (SVM)**: The SVM model that performed best had the 'linear' kernel with a C value of 1.0 and 'auto' gamma setting. This indicates a model that balances margin and misclassification error, without the need for the complexity of a non-linear kernel.\n",
    "\n",
    "- **Decision Tree**: The best decision tree model had a maximum depth of 3. This suggests that a simpler model, which avoids overfitting by not going too deep into the tree, was sufficient to capture the relevant patterns in the data.\n",
    "\n",
    "- **K-Nearest Neighbors (KNN)**: The KNN model that yielded the best results had 15 neighbors. This points to an intermediate complexity that balances between smoothing out the noise and capturing sufficient detail from the dataset.\n",
    "\n",
    "We shall see their performance in the following table:\n",
    "\n",
    "\n",
    "| Best Model Configuration          | Accuracy         | Sensitivity      | Specificity      | PPV              | NPV              | F_Score          | Err_Rate         |\n",
    "|---------------------|------------------|------------------|------------------|------------------|------------------|------------------|------------------|\n",
    "| ANN (Model 5)       | 0.948 ± 0.036    | 0.948 ± 0.036    | 0.979 ± 0.013    | 0.959 ± 0.027    | 0.969 ± 0.025    | 0.948 ± 0.036    | 0.052 ± 0.036    |\n",
    "| SVM (Model 4)       | 0.953 ± 0.051    | 0.953 ± 0.051    | 0.980 ± 0.022    | 0.964 ± 0.034    | 0.973 ± 0.033    | 0.953 ± 0.051    | 0.047 ± 0.051    |\n",
    "| Decision Tree (Model 1) | 0.927 ± 0.043 | 0.927 ± 0.043    | 0.967 ± 0.021    | 0.940 ± 0.032    | 0.955 ± 0.032    | 0.927 ± 0.043    | 0.073 ± 0.043    |\n",
    "| KNN (Model 3)       | 0.96 ± 0.015     | 0.96 ± 0.015     | 0.98 ± 0.009     | 0.965 ± 0.011    | 0.978 ± 0.015    | 0.96 ± 0.015     | 0.04 ± 0.015     |\n",
    "\n",
    "### Best performing model \n",
    "\n",
    "The KNN model outperforms the others with the highest accuracy of 0.96 and the lowest standard deviation (0.015). It appears to offer the best compromise between bias and variance, which is a key factor in its superior performance in this evaluation. It's worth noting, though, that the ideal model could vary depending on the specific data and the context of the problem. While accuracy is a critical metric, and we believe is a good metric for our use case, it may not be the sole criterion for success in other cases, especially when the consequences of false positives and false negatives differ significantly. In such scenarios, other metrics like sensitivity or specificity may be more relevant for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
